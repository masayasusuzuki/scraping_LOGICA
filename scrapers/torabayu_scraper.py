import streamlit as st
import requests
from bs4 import BeautifulSoup
import urllib.parse
import time
import random
import html
import re
import pandas as pd
import os
import gspread
from google.oauth2 import service_account

class TorabayuScraper:
    """ã¨ã‚‰ã°ãƒ¼ã‚†æ±‚äººã‚µã‚¤ãƒˆã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ï¼ˆåœ°åŸŸå¯¾å¿œï¼‰"""
    
    def __init__(self, region="tokyo"):
        self.region = region
        self.region_urls = {
            "tokyo": "https://toranet.jp/prefectures/tokyo/",
            "kanagawa": "https://toranet.jp/prefectures/kanagawa/",
            "chiba": "https://toranet.jp/prefectures/chiba/",
            "saitama": "https://toranet.jp/prefectures/saitama/"
        }
        self.base_url = self.region_urls.get(region, self.region_urls["tokyo"])
        self.session = requests.Session()
        self.session.headers.update(self.get_headers())
        
    def get_headers(self):
        """ãƒªã‚¯ã‚¨ã‚¹ãƒˆãƒ˜ãƒƒãƒ€ãƒ¼ã‚’ç”Ÿæˆ"""
        return {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',
            'Accept-Language': 'ja,en-US;q=0.9,en;q=0.8',
            'Referer': 'https://toranet.jp/',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Cache-Control': 'max-age=0',
        }
    
    def is_valid_job_url(self, url):
        """æœ‰åŠ¹ãªæ±‚äººURLã‹ãƒã‚§ãƒƒã‚¯"""
        if not url:
            return False
        
        invalid_paths = ['favorite_jobs', 'login', 'register', 'contact', 'about']
        for path in invalid_paths:
            if path in url:
                return False
        
        valid = (
            ('toranet.jp' in url and ('job' in url or 'kyujin' in url or 'prefectures' in url)) or
            ('job_detail' in url)
        )
        
        return valid
    
    def make_request(self, url, max_retries=5, timeout=30):
        """ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’é€ä¿¡ï¼ˆãƒªãƒˆãƒ©ã‚¤æ©Ÿèƒ½ä»˜ãï¼‰"""
        if not self.is_valid_job_url(url):
            return None, f"ç„¡åŠ¹ãªURL: {url}"
        
        for attempt in range(max_retries):
            try:
                if attempt > 0:
                    sleep_time = random.uniform(3, 7)
                    time.sleep(sleep_time)
                else:
                    time.sleep(random.uniform(0.5, 1.5))
                
                response = self.session.get(url, timeout=timeout)
                response.raise_for_status()
                return response, None
                
            except requests.exceptions.HTTPError as e:
                if e.response.status_code == 503 and attempt < max_retries - 1:
                    continue
                return None, f"HTTPã‚¨ãƒ©ãƒ¼: {e.response.status_code} - {e}"
            except requests.exceptions.Timeout:
                if attempt < max_retries - 1:
                    continue
                return None, "ãƒªã‚¯ã‚¨ã‚¹ãƒˆãŒã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã—ã¾ã—ãŸ"
            except requests.exceptions.RequestException as e:
                if attempt < max_retries - 1:
                    continue
                return None, f"ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {str(e)}"
        
        return None, "æœ€å¤§å†è©¦è¡Œå›æ•°ã«é”ã—ã¾ã—ãŸ"
    
    def extract_phone_number(self, text):
        """ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰é›»è©±ç•ªå·ã‚’æŠ½å‡ºï¼ˆéƒµä¾¿ç•ªå·ã¨ã®èª¤èªè­˜ã‚’é˜²æ­¢ï¼‰"""
        if not text:
            return None
        
        # éƒµä¾¿ç•ªå·ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’äº‹å‰ã«é™¤å¤–
        # 7æ¡ã®æ•°å­—ï¼ˆ123-4567å½¢å¼ï¼‰ã‚„ã€’ãƒãƒ¼ã‚¯ä»˜è¿‘ã®æ•°å­—ã¯é™¤å¤–
        postal_patterns = [
            r'ã€’\s*\d{3}[-\s]?\d{4}',
            r'éƒµä¾¿ç•ªå·[ï¼š:\s]*\d{3}[-\s]?\d{4}',
            r'^\d{3}-\d{4}$',  # å˜ä½“ã§å­˜åœ¨ã™ã‚‹7æ¡ç•ªå·
        ]
        
        # é›»è©±ç•ªå·ã®å³å¯†ãªãƒ‘ã‚¿ãƒ¼ãƒ³å®šç¾©
        phone_patterns = [
            # ãƒ•ãƒªãƒ¼ãƒ€ã‚¤ãƒ¤ãƒ«ãƒ»ç‰¹ç•ªï¼ˆ0120, 0800, 0570, 050, 0180ï¼‰
            r'0120[-\s]?\d{3}[-\s]?\d{3}',
            r'0800[-\s]?\d{3}[-\s]?\d{4}',
            r'0570[-\s]?\d{3}[-\s]?\d{3}',
            r'050[-\s]?\d{4}[-\s]?\d{4}',
            r'0180[-\s]?\d{3}[-\s]?\d{3}',
            
            # æºå¸¯é›»è©±ï¼ˆ090, 080, 070, 060, 020ï¼‰
            r'0[987206]0[-\s]?\d{4}[-\s]?\d{4}',
            
            # å›ºå®šé›»è©±ï¼ˆ2æ¡å¸‚å¤–å±€ç•ªï¼š03, 06ãªã©ï¼‰
            r'0[3-6][-\s]?\d{4}[-\s]?\d{4}',
            
            # å›ºå®šé›»è©±ï¼ˆ3æ¡å¸‚å¤–å±€ç•ªï¼š011, 045, 052ãªã©ï¼‰
            r'0[1-9]\d[-\s]?\d{3,4}[-\s]?\d{4}',
            
            # å›ºå®šé›»è©±ï¼ˆ4æ¡å¸‚å¤–å±€ç•ªï¼šåœ°æ–¹ç•ªå·ï¼‰
            r'0[1-9]\d{2}[-\s]?\d{2,3}[-\s]?\d{4}',
            
            # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ä»˜ããƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆã‚ˆã‚Šå³å¯†ï¼‰
            r'é›»è©±ç•ªå·[ï¼š:\s]*([0-9ï¼-ï¼™][0-9ï¼-ï¼™\-\(\)\s]{8,13})',
            r'TEL[ï¼š:\s]*([0-9ï¼-ï¼™][0-9ï¼-ï¼™\-\(\)\s]{8,13})',
            r'Tel[ï¼š:\s]*([0-9ï¼-ï¼™][0-9ï¼-ï¼™\-\(\)\s]{8,13})',
            r'é›»è©±[ï¼š:\s]*([0-9ï¼-ï¼™][0-9ï¼-ï¼™\-\(\)\s]{8,13})',
        ]
        
        # ã¾ãšéƒµä¾¿ç•ªå·ã‚’ãƒã‚§ãƒƒã‚¯ã—ã¦é™¤å¤–
        for postal_pattern in postal_patterns:
            if re.search(postal_pattern, text):
                # éƒµä¾¿ç•ªå·ãŒå«ã¾ã‚Œã¦ã„ã‚‹å ´åˆã¯ã€ãã®éƒ¨åˆ†ã‚’é™¤å¤–ã—ã¦å‡¦ç†
                text = re.sub(postal_pattern, '', text)
        
        # é›»è©±ç•ªå·ã‚’æŠ½å‡º
        for pattern in phone_patterns:
            matches = re.search(pattern, text, re.IGNORECASE)
            if matches:
                phone = matches.group(1) if len(matches.groups()) > 0 else matches.group(0)
                
                # æ•°å­—ã®ã¿æŠ½å‡ºã—ã¦æ¤œè¨¼
                digits_only = re.sub(r'[^\d]', '', phone)
                
                # é›»è©±ç•ªå·ã¨ã—ã¦æœ‰åŠ¹ãªæ¡æ•°ã‹ãƒã‚§ãƒƒã‚¯ï¼ˆ10æ¡ã¾ãŸã¯11æ¡ï¼‰
                if len(digits_only) not in [10, 11]:
                    continue
                
                # 0ã§å§‹ã¾ã‚‹ã“ã¨ã‚’ç¢ºèª
                if not digits_only.startswith('0'):
                    continue
                
                # éƒµä¾¿ç•ªå·å½¢å¼ï¼ˆ7æ¡ï¼‰ã‚’é™¤å¤–
                if len(digits_only) == 7:
                    continue
                
                # ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆèª¿æ•´
                if digits_only.startswith('0120') and len(digits_only) == 10:
                    return f"{digits_only[:4]}-{digits_only[4:7]}-{digits_only[7:]}"
                elif len(digits_only) == 11:
                    # æºå¸¯é›»è©±å½¢å¼
                    return f"{digits_only[:3]}-{digits_only[3:7]}-{digits_only[7:]}"
                elif len(digits_only) == 10:
                    # å›ºå®šé›»è©±å½¢å¼
                    if digits_only.startswith('03') or digits_only.startswith('06'):
                        return f"{digits_only[:2]}-{digits_only[2:6]}-{digits_only[6:]}"
                    else:
                        return f"{digits_only[:3]}-{digits_only[3:6]}-{digits_only[6:]}"
                
                return phone
        
        return None
    
    def extract_representative(self, text):
        """ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ä»£è¡¨è€…åã‚’æŠ½å‡º"""
        if not text:
            return ""
        
        text = re.sub(r'<[^>]+>', ' ', text)
        text = re.sub(r'[\[\]ã€ã€‘ï¼»ï¼½()ï¼ˆï¼‰ã€Œã€ã€ã€â‰ªâ‰«<>ï¼œï¼""\'\']+', ' ', text)
        text = re.sub(r'\s+', ' ', text).strip()
        
        patterns = [
            r'ä»£è¡¨è€…\s*[\n\r:ï¼š]*\s*([^\n\rï¼ˆ(ã€ï¼»[{]+)',
            r'ä»£è¡¨å–ç· å½¹\s*[\n\r:ï¼š]*\s*([^\n\rï¼ˆ(ã€ï¼»[{]+)',
            r'é™¢é•·\s*[\n\r:ï¼š]*\s*([^\n\r:ï¼šï¼ˆ(ã€ï¼»[{]+)',
            r'ç†äº‹é•·\s*[\n\r:ï¼š]*\s*([^\n\r:ï¼šï¼ˆ(ã€ï¼»[{]+)',
        ]
        
        for pattern in patterns:
            matches = re.search(pattern, text, re.DOTALL)
            if matches and matches.group(1):
                name = matches.group(1).strip()
                if name and name != "è€…" and len(name) > 1:
                    name = re.sub(r'æ‰€åœ¨ä½æ‰€.*$', '', name)
                    name = re.sub(r'ä½æ‰€.*$', '', name)
                    name = re.sub(r'[0-9ï¼-ï¼™]{5,}.*$', '', name)
                    name = name.strip()
                    if len(name) > 1:
                        return name
        
        return ""
    
    def extract_address(self, text):
        """ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ä½æ‰€ã‚’æŠ½å‡º"""
        if not text:
            return ""
        
        text = re.sub(r'<[^>]+>', ' ', text)
        text = re.sub(r'\s+', ' ', text).strip()
        
        patterns = [
            r'å‹¤å‹™åœ°\s*[\n\r:ï¼š]*\s*([^\n\r]{5,100})',
            r'æ‰€åœ¨ä½æ‰€\s*[\n\r:ï¼š]*\s*([^\n\r]{5,100})',
            r'æ‰€åœ¨åœ°\s*[\n\r:ï¼š]*\s*([^\n\r]{5,100})',
            r'ã€’\d{3}-\d{4}\s*([^\n\r]{5,100})',
        ]
        
        for pattern in patterns:
            matches = re.search(pattern, text, re.DOTALL)
            if matches and matches.group(1):
                address = matches.group(1).strip()
                address = re.sub(r'\s+', ' ', address)
                return address
        
        return ""
    
    def clean_facility_name(self, name):
        """æ–½è¨­åã‚’ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°"""
        if not name:
            return "æƒ…å ±ãªã—"
        
        # ä¸è¦ãªãƒ†ã‚­ã‚¹ãƒˆã‚’å‰Šé™¤
        name = re.sub(r'ã®æ±‚äººè©³ç´°$', '', name)
        name = re.sub(r'ã®æ±‚äººæƒ…å ±$', '', name)
        name = re.sub(r'ã®æ±‚äºº$', '', name)
        name = re.sub(r'è©³ç´°$', '', name)
        name = re.sub(r'ã€.*?ã€‘', '', name)
        name = re.sub(r'ã€Œ.*?ã€', '', name)
        name = re.sub(r'\(.*?\)', '', name)
        name = re.sub(r'ï¼ˆ.*?ï¼‰', '', name)
        name = re.sub(r'ã¨ã‚‰ã°ãƒ¼ã‚†', '', name)
        
        # è·ç¨®åã‚’å‰Šé™¤
        job_patterns = [
            'çœ‹è­·å¸«', 'ä»‹è­·å£«', 'åŒ»å¸«', 'è–¬å‰¤å¸«', 'ä¿è‚²å£«', 'æ „é¤Šå£«', 
            'æ­£ç¤¾å“¡', 'ãƒ‘ãƒ¼ãƒˆ', 'ã‚¢ãƒ«ãƒã‚¤ãƒˆ', 'å¥‘ç´„ç¤¾å“¡', 'æ´¾é£'
        ]
        for pattern in job_patterns:
            name = re.sub(f'{pattern}(å‹Ÿé›†)?$', '', name)
            name = re.sub(f'^{pattern}', '', name)
        
        name = re.sub(r'\s+', ' ', name).strip()
        name = re.sub(r'^[ã€,.:ï¼šãƒ»]+', '', name)
        name = re.sub(r'[ã€,.:ï¼šãƒ»]+$', '', name)
        
        return name.strip()
    
    def find_all_job_links(self, soup, search_url, max_jobs):
        """æ±‚äººãƒªãƒ³ã‚¯ã‚’å…¨ã¦å–å¾—"""
        all_links = []
        
        for a_tag in soup.find_all('a', href=True):
            href = a_tag.get('href')
            if not href:
                continue
                
            if not href.startswith('http'):
                if href.startswith('/'):
                    href = f"https://toranet.jp{href}"
                else:
                    href = f"https://toranet.jp/{href}"
            
            if self.is_valid_job_url(href):
                text = a_tag.get_text().strip()
                all_links.append({
                    'href': href,
                    'text': text,
                    'contains_detail_text': 'è©³ç´°' in text or 'detail' in href.lower() or 'æ±‚äºº' in text
                })
        
        # è©³ç´°ãƒªãƒ³ã‚¯ã‚’å„ªå…ˆ
        detail_links = [link for link in all_links if link['contains_detail_text']]
        pattern_links = [link for link in all_links if re.search(r'job.*detail|kyujin|recruit', link['href'])]
        
        combined_links = []
        seen_urls = set()
        
        for link in detail_links + pattern_links:
            if link['href'] not in seen_urls:
                combined_links.append(link)
                seen_urls.add(link['href'])
        
        for link in all_links:
            if link['href'] not in seen_urls and search_url not in link['href'] and 'toranet.jp' in link['href']:
                combined_links.append(link)
                seen_urls.add(link['href'])
        
        return [link['href'] for link in combined_links][:max_jobs]
    
    def get_job_listings(self, keyword, specific_area=None, max_jobs=10):
        """æ±‚äººä¸€è¦§ã‚’å–å¾—ï¼ˆåœ°åŸŸå¯¾å¿œï¼‰"""
        encoded_keyword = urllib.parse.quote(keyword)
        
        # åœ°åŸŸã”ã¨ã®ãƒ™ãƒ¼ã‚¹URLæ§‹ç¯‰
        if specific_area and specific_area != "å…¨åŸŸ":
            # ç‰¹å®šåœ°åŸŸãŒé¸æŠã•ã‚ŒãŸå ´åˆã¯ã€ãã®åœ°åŸŸã§æ¤œç´¢
            area_encoded = urllib.parse.quote(specific_area)
            base_search_url = f"{self.base_url}job_search/kw/{encoded_keyword}/area/{area_encoded}"
        else:
            # å…¨åŸŸã¾ãŸã¯åœ°åŸŸæœªé¸æŠã®å ´åˆ
            base_search_url = f"{self.base_url}job_search/kw/{encoded_keyword}"
        
        all_job_links = []
        current_page = 1
        max_pages = 10
        
        while len(all_job_links) < max_jobs and current_page <= max_pages:
            if current_page == 1:
                search_url = base_search_url
            else:
                search_url = f"{base_search_url}/page/{current_page}"
            
            response, error = self.make_request(search_url)
            if error:
                if current_page > 1:
                    break
                else:
                    return None, error, search_url
            
            try:
                soup = BeautifulSoup(response.text, 'html.parser')
                page_job_links = self.find_all_job_links(soup, search_url, max_jobs)
                
                if not page_job_links:
                    if current_page == 1:
                        # æ¤œç´¢ãƒšãƒ¼ã‚¸è‡ªä½“ãŒæ±‚äººè©³ç´°ã‹ãƒã‚§ãƒƒã‚¯
                        if any(tag.name in ['h1', 'h2'] and ('æ±‚äººæƒ…å ±' in tag.text or 'ä»•äº‹å†…å®¹' in tag.text) 
                               for tag in soup.find_all(['h1', 'h2'])):
                            return [search_url], None, search_url
                        return None, "æ±‚äººãƒªãƒ³ã‚¯ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ", search_url
                    else:
                        break
                
                existing_links = set(all_job_links)
                for link in page_job_links:
                    if link not in existing_links and len(all_job_links) < max_jobs:
                        all_job_links.append(link)
                        existing_links.add(link)
                
                current_page += 1
                
                if len(all_job_links) >= max_jobs:
                    break
                    
                time.sleep(random.uniform(1.0, 3.0))
                
            except Exception as e:
                if current_page == 1:
                    return None, f"ãƒ‘ãƒ¼ã‚¹ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}", search_url
                else:
                    break
        
        return all_job_links, None, base_search_url
    
    def get_job_details(self, detail_url):
        """æ±‚äººè©³ç´°æƒ…å ±ã‚’å–å¾—"""
        if not self.is_valid_job_url(detail_url):
            return None, f"ç„¡åŠ¹ãªè©³ç´°ãƒšãƒ¼ã‚¸URL: {detail_url}"
        
        time.sleep(random.uniform(0.3, 1.0))
        
        response, error = self.make_request(detail_url)
        if error:
            return None, error
        
        try:
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # æ–½è¨­åæŠ½å‡º
            facility_name = "æƒ…å ±ãªã—"
            facility_name_selectors = [
                'div.corpNameWrap > span', 
                'div.corpName', 
                'h1.company-name',
                'div.company-name',
                'h1', 'h2',
                'div.corpInfo',
                'span.name',
                '.corp-name',
                '.company'
            ]
            
            for selector in facility_name_selectors:
                facility_name_element = soup.select_one(selector)
                if facility_name_element:
                    facility_name = self.clean_facility_name(facility_name_element.text.strip())
                    break
            
            # ã‚¿ã‚¤ãƒˆãƒ«ã‹ã‚‰ã®æŠ½å‡ºï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
            if facility_name == "æƒ…å ±ãªã—":
                title_tag = soup.find('title')
                if title_tag:
                    title_text = title_tag.text.strip()
                    for separator in ['|', '-', 'ï¼š', ':', 'ï¼', '/']: 
                        if separator in title_text:
                            parts = title_text.split(separator)
                            for part in parts:
                                part = part.strip()
                                if 5 < len(part) < 50 and not re.search(r'æ±‚äºº|å‹Ÿé›†|æ¡ç”¨|ã¨ã‚‰ã°ãƒ¼ã‚†|è»¢è·', part):
                                    facility_name = self.clean_facility_name(part)
                                    break
                            if facility_name != "æƒ…å ±ãªã—":
                                break
            
            # ä»£è¡¨è€…æŠ½å‡º
            representative = ""
            representative_elements = soup.select('p.styles_content__HWIR6')
            for element in representative_elements:
                prev_el = element.find_previous()
                if prev_el and prev_el.name == 'h3' and 'ä»£è¡¨è€…' in prev_el.text:
                    content = element.text.strip()
                    if content and content != "è€…" and len(content) > 1:
                        representative = content
                        break
            
            if not representative:
                page_text = soup.get_text()
                representative = self.extract_representative(page_text)
            
            # å‹¤å‹™åœ°æŠ½å‡º
            location = ""
            location_elements = soup.select('p.styles_content__HWIR6')
            for element in location_elements:
                prev_el = element.find_previous()
                if prev_el and prev_el.name == 'h3' and 'å‹¤å‹™åœ°' in prev_el.text:
                    content = element.text.strip()
                    if content and len(content) > 5:
                        location = content
                        break
            
            if not location:
                page_text = soup.get_text()
                location = self.extract_address(page_text)
            
            # é›»è©±ç•ªå·æŠ½å‡º
            phone_number = "æƒ…å ±ãªã—"
            phone_elements = soup.select('p.styles_content__HWIR6')
            for element in phone_elements:
                prev_el = element.find_previous()
                if prev_el and prev_el.name == 'h3' and ('ä»£è¡¨é›»è©±ç•ªå·' in prev_el.text or 'é›»è©±ç•ªå·' in prev_el.text):
                    content = element.text.strip()
                    if content:
                        digits = re.sub(r'[^\d]', '', content)
                        if digits and len(digits) >= 10:
                            if digits.startswith('0120'):
                                phone_number = f"{digits[:4]}-{digits[4:7]}-{digits[7:10]}"
                            elif len(digits) == 10:
                                phone_number = f"{digits[:2]}-{digits[2:6]}-{digits[6:10]}"
                            elif len(digits) == 11:
                                phone_number = f"{digits[:3]}-{digits[3:7]}-{digits[7:11]}"
                            else:
                                phone_number = digits
                        break
            
            if phone_number == "æƒ…å ±ãªã—":
                page_text = soup.get_text()
                extracted_number = self.extract_phone_number(page_text)
                if extracted_number:
                    phone_number = extracted_number
            
            # æ¥­å‹™å†…å®¹æŠ½å‡º
            job_description = "æƒ…å ±ãªã—"
            job_sections = soup.find_all('h3', string=lambda s: s and ('è·ç¨®/ä»•äº‹å†…å®¹' in s or 'ä»•äº‹å†…å®¹' in s))
            for section in job_sections:
                parent_th = section.find_parent('th')
                if parent_th:
                    td = parent_th.find_next_sibling('td')
                    if td:
                        job_description = td.text.strip()
                        break
            
            if job_description == "æƒ…å ±ãªã—":
                job_content_elements = soup.select('td.styles_content__cGhMI.styles_commonContent__NDgRD.styles_recruitCol__rbAHs')
                for element in job_content_elements:
                    prev_th = element.find_previous('th')
                    if prev_th and prev_th.find('p') and ('è·ç¨®/ä»•äº‹å†…å®¹' in prev_th.text or 'ä»•äº‹å†…å®¹' in prev_th.text):
                        job_description = element.text.strip()
                        break
            
            # ä½æ‰€ã®ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
            if location and facility_name and facility_name != "æƒ…å ±ãªã—" and facility_name in location:
                parts = location.split(facility_name)
                if len(parts) > 1:
                    location = parts[1].strip()
                    location = re.sub(r'^[ã€,:ï¼š\s]+', '', location)
            
            location = re.sub(r'^å‹¤å‹™åœ°[ï¼š:]\s*', '', location)
            location = re.sub(r'ä»£è¡¨é›»è©±.*$', '', location)
            location = re.sub(r'äº‹æ¥­å†…å®¹.*$', '', location)
            location = location.strip()
            
            # ä»£è¡¨è€…ã®ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
            if representative:
                representative = re.sub(r'æ‰€åœ¨ä½æ‰€.*$', '', representative)
                representative = re.sub(r'ä½æ‰€.*$', '', representative)
                representative = re.sub(r'[0-9ï¼-ï¼™]{5,}.*$', '', representative)
                representative = re.sub(r'æ±äº¬éƒ½.*$', '', representative)
                representative = re.sub(r'ä»£è¡¨é›»è©±.*$', '', representative)
                representative = representative.strip()
            
            short_description = job_description[:100] + "..." if len(job_description) > 100 else job_description
            
            return {
                "facility_name": facility_name,
                "representative": representative,
                "location": location,
                "phone_number": phone_number,
                "job_description": job_description,
                "short_description": short_description,
                "source_url": detail_url
            }, None
            
        except Exception as e:
            return None, f"è©³ç´°æƒ…å ±ã®è§£æä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}"
    
    def scrape_jobs(self, keyword, specific_area=None, max_jobs=10, progress_callback=None):
        """æ±‚äººæƒ…å ±ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ï¼ˆåœ°åŸŸå¯¾å¿œï¼‰"""
        try:
            # æ±‚äººãƒªãƒ³ã‚¯å–å¾—
            job_links, error, search_url = self.get_job_listings(keyword, specific_area, max_jobs)
            
            if error:
                return None, error
            
            if not job_links:
                return None, "æ±‚äººãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ"
            
            job_list = []
            total_jobs = len(job_links)
            error_count = 0
            error_limit = min(total_jobs // 2, 50)
            
            for idx, link in enumerate(job_links):
                if progress_callback:
                    progress_callback(idx + 1, total_jobs)
                
                job_details, error = self.get_job_details(link)
                
                if error:
                    error_count += 1
                    if error_count >= error_limit:
                        break
                elif job_details:
                    job_list.append(job_details)
            
            return job_list, None
            
        except Exception as e:
            return None, f"ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}"


class TorabayuUI:
    """ã¨ã‚‰ã°ãƒ¼ã‚†ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ã®UIï¼ˆåœ°åŸŸå¯¾å¿œï¼‰"""
    
    def __init__(self, region="tokyo"):
        self.region = region
        self.region_names = {
            "tokyo": "æ±äº¬",
            "kanagawa": "ç¥å¥ˆå·", 
            "chiba": "åƒè‘‰",
            "saitama": "åŸ¼ç‰"
        }
        self.scraper = TorabayuScraper(region)
        
        # åœ°åŸŸã”ã¨ã®ä¸»è¦ã‚¨ãƒªã‚¢
        self.area_options = {
            "tokyo": ["å…¨åŸŸ", "23åŒº", "23åŒºå¤–", "æ–°å®¿åŒº", "æ¸‹è°·åŒº", "æ¸¯åŒº", "ä¸­å¤®åŒº", "åƒä»£ç”°åŒº", "è±Šå³¶åŒº", "æ–‡äº¬åŒº", "å°æ±åŒº", "å“å·åŒº", "å¤§ç”°åŒº", "ä¸–ç”°è°·åŒº", "ç›®é»’åŒº"],
            "kanagawa": ["å…¨åŸŸ", "æ¨ªæµœå¸‚", "å·å´å¸‚", "ç›¸æ¨¡åŸå¸‚", "ç¥å¥ˆå·çœŒå¤®/åŒ—éƒ¨", "æ¨ªé ˆè³€/ä¸‰æµ¦", "æ¹˜å—/è¥¿æ¹˜"],
            "chiba": ["å…¨åŸŸ", "åƒè‘‰å¸‚", "èˆ¹æ©‹å¸‚", "æ¾æˆ¸å¸‚", "å¸‚å·å¸‚", "æŸå¸‚", "å¸‚åŸå¸‚", "å…«åƒä»£å¸‚", "æµå±±å¸‚", "æµ¦å®‰å¸‚"],
            "saitama": ["å…¨åŸŸ", "ã•ã„ãŸã¾å¸‚", "å·å£å¸‚", "å·è¶Šå¸‚", "æ‰€æ²¢å¸‚", "è¶Šè°·å¸‚", "è‰åŠ å¸‚", "æ˜¥æ—¥éƒ¨å¸‚", "ç†Šè°·å¸‚", "ä¸Šå°¾å¸‚"]
        }
    
    def render_ui(self):
        """UIã‚’æç”»ã—ã¦çµæœã‚’è¿”ã™"""
        region_name = self.region_names.get(self.region, self.region)
        st.header(f"ã¨ã‚‰ã°ãƒ¼ã‚† {region_name}ï¼šæ¤œç´¢æ¡ä»¶")
        
        # ãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰
        debug_mode = st.sidebar.checkbox("ãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰", key=f"torabayu_{self.region}_debug")
        
        # æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
        search_keyword = st.text_input(
            "è·ç¨®åã‚„æ–½è¨­åã‚’å…¥åŠ›ã—ã¦ãã ã•ã„", 
            placeholder="çœ‹è­·å¸«ãªã©",
            key=f"torabayu_{self.region}_keyword"
        )
        
        # å–å¾—ä»¶æ•°è¨­å®š
        max_jobs = st.selectbox(
            "å–å¾—ä»¶æ•°ã‚’é¸æŠ",
            [10, 20, 50, 100, 200, 300],
            index=0,  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§10ä»¶
            key=f"torabayu_{self.region}_max_jobs",
            help="å–å¾—ã™ã‚‹æ±‚äººã®æœ€å¤§ä»¶æ•°ã‚’é¸æŠã—ã¦ãã ã•ã„"
        )
        
        # å®Ÿè¡Œéƒ¨
        start_button = st.button("æ±‚äººã‚’æ¢ã™", type="primary", key=f"torabayu_{self.region}_start")
        
        # æ¤œç´¢å®Ÿè¡Œ
        if search_keyword and start_button:
            with st.spinner("æ±‚äººæƒ…å ±ã‚’å–å¾—ä¸­... ã—ã°ã‚‰ããŠå¾…ã¡ãã ã•ã„"):
                # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼
                progress_bar = st.progress(0)
                status_text = st.empty()
                
                def progress_callback(current, total):
                    progress_bar.progress(current / total)
                    status_text.text(f"æ±‚äººæƒ…å ±ã‚’å–å¾—ä¸­... ({current}/{total})")
                
                # åœ°åŸŸæŒ‡å®šã¯å‰Šé™¤ã•ã‚ŒãŸã®ã§Noneã‚’æ¸¡ã™
                job_list, error = self.scraper.scrape_jobs(search_keyword, None, max_jobs, progress_callback)
                
                # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼ã‚’ã‚¯ãƒªã‚¢
                progress_bar.empty()
                status_text.empty()
                
                if error:
                    st.error(error)
                    return None
                
                if not job_list:
                    st.warning("æ±‚äººæƒ…å ±ã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")
                    return None
                
                # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã«å¤‰æ›
                df_data = []
                for job in job_list:
                    df_data.append({
                        'æ–½è¨­å': job['facility_name'],
                        'ä»£è¡¨è€…å': job['representative'] if job['representative'] else '',
                        'å‹¤å‹™åœ°': job['location'] if job['location'] else '',
                        'æ±‚äººURL': job['source_url'],
                        'é›»è©±ç•ªå·': job['phone_number'] if job['phone_number'] != "æƒ…å ±ãªã—" else '',
                        'ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹': '',  # ç¾åœ¨æœªå®Ÿè£…
                        'æ¥­å‹™å†…å®¹': job['short_description']
                    })
                
                df = pd.DataFrame(df_data)
                
                # Google Sheetsã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆæ©Ÿèƒ½
                self.render_google_sheets_export(job_list)
                
                return df
        
        else:
            if not search_keyword:
                st.info("è·ç¨®åã‚„æ–½è¨­åã‚’å…¥åŠ›ã—ã¦æ±‚äººã‚’æ¢ã—ã¦ãã ã•ã„ã€‚")
            return None
    
    def render_google_sheets_export(self, job_list):
        """Google Sheetsã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆæ©Ÿèƒ½ã‚’æç”»"""
        st.subheader("ğŸ“Š Google Sheetsã¸ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ")
        
        # ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆURLå…¥åŠ›
        spreadsheet_url = st.text_input(
            "Google Sheetsã®URL",
            placeholder="https://docs.google.com/spreadsheets/d/your-sheet-id/edit",
            key=f"torabayu_{self.region}_sheets_url"
        )
        
        if spreadsheet_url:
            # ã‚·ãƒ¼ãƒˆåå…¥åŠ›
            region_name = self.region_names.get(self.region, self.region)
            sheet_name = st.text_input(
                "ã‚·ãƒ¼ãƒˆå",
                value=f"ã¨ã‚‰ã°ãƒ¼ã‚†{region_name}_{pd.Timestamp.now().strftime('%Y%m%d_%H%M%S')}",
                key=f"torabayu_{self.region}_sheet_name"
            )
            
            # ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆãƒœã‚¿ãƒ³
            if st.button("ğŸ“¤ Google Sheetsã«ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ", type="secondary", key=f"torabayu_{self.region}_export"):
                with st.spinner("Google Sheetsã«å‡ºåŠ›ä¸­..."):
                    success, message = self.export_to_google_sheets(job_list, spreadsheet_url, sheet_name)
                    
                    if success:
                        st.success(message)
                    else:
                        st.error(message)
        else:
            st.info("ğŸ“‹ Google Sheetsã®URLã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
    
    def export_to_google_sheets(self, job_list, spreadsheet_url, sheet_name=None):
        """Google Sheetsã«ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ"""
        try:
            # èªè¨¼è¨­å®š
            credentials_path = "credentials/service_account.json"
            
            # èªè¨¼ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã™ã‚‹ã‹ç¢ºèª
            if not os.path.exists(credentials_path):
                return False, "Google APIã®èªè¨¼ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚credentials/service_account.jsonã‚’é…ç½®ã—ã¦ãã ã•ã„ã€‚"
            
            # Google APIã®èªè¨¼æƒ…å ±ã‚’èª­ã¿è¾¼ã‚€
            credentials = service_account.Credentials.from_service_account_file(
                credentials_path,
                scopes=["https://www.googleapis.com/auth/spreadsheets", "https://www.googleapis.com/auth/drive"]
            )
            
            # Google Sheetsã«æ¥ç¶š
            client = gspread.authorize(credentials)
            
            # ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã®URLã‹ã‚‰å¯¾è±¡ã‚’å–å¾—
            try:
                spreadsheet = client.open_by_url(spreadsheet_url)
            except Exception as e:
                return False, f"ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã‚’é–‹ã‘ã¾ã›ã‚“ã§ã—ãŸ: {str(e)}"
            
            # ã‚·ãƒ¼ãƒˆåãŒæŒ‡å®šã•ã‚Œã¦ã„ãªã„å ´åˆã¯æ–°ã—ã„ã‚·ãƒ¼ãƒˆã‚’ä½œæˆ
            if not sheet_name:
                region_name = self.region_names.get(self.region, self.region)
                sheet_name = f"ã¨ã‚‰ã°ãƒ¼ã‚†{region_name}_{time.strftime('%Y%m%d_%H%M%S')}"
            
            try:
                worksheet = spreadsheet.worksheet(sheet_name)
            except Exception:
                # æŒ‡å®šã—ãŸã‚·ãƒ¼ãƒˆãŒå­˜åœ¨ã—ãªã„å ´åˆã¯æ–°è¦ä½œæˆ
                try:
                    worksheet = spreadsheet.add_worksheet(title=sheet_name, rows=1000, cols=20)
                except Exception as e:
                    return False, f"ã‚·ãƒ¼ãƒˆã‚’å–å¾—ã¾ãŸã¯ä½œæˆã§ãã¾ã›ã‚“ã§ã—ãŸ: {str(e)}"
            
            # ãƒ˜ãƒƒãƒ€ãƒ¼è¡Œã‚’è¨­å®š
            headers = [
                "æ–½è¨­å", "ä»£è¡¨è€…å", "å‹¤å‹™åœ°", "æ±‚äººURL", "é›»è©±ç•ªå·", "ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹", "æ¥­å‹™å†…å®¹"
            ]
            
            # ã‚·ãƒ¼ãƒˆã‚’ã‚¯ãƒªã‚¢ã—ã¦æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ã‚’æ›¸ãè¾¼ã‚€
            worksheet.clear()
            worksheet.append_row(headers)
            
            # ãƒ‡ãƒ¼ã‚¿è¡Œã®ä½œæˆ
            data_rows = []
            for job in job_list:
                # ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
                representative = job.get('representative', '')
                if representative:
                    representative = re.sub(r'æ‰€åœ¨ä½æ‰€.*$', '', representative)
                    representative = re.sub(r'ä½æ‰€.*$', '', representative)
                    representative = re.sub(r'[0-9ï¼-ï¼™]{5,}.*$', '', representative)
                    representative = re.sub(r'ä»£è¡¨é›»è©±.*$', '', representative)
                    representative = representative.strip()
                
                location = job.get('location', '')
                if location:
                    location = re.sub(r'^å‹¤å‹™åœ°[ï¼š:]\s*', '', location)
                    location = re.sub(r'ä»£è¡¨é›»è©±.*$', '', location)
                    location = re.sub(r'äº‹æ¥­å†…å®¹.*$', '', location)
                    location = location.strip()
                
                phone_number = job.get('phone_number', '')
                if phone_number and phone_number != "æƒ…å ±ãªã—":
                    phone_number = re.sub(r'[^\d\-\(\)]', '', phone_number).strip()
                else:
                    phone_number = ""
                
                row = [
                    job.get('facility_name', ''),
                    representative,
                    location,
                    job.get('source_url', ''),
                    phone_number,
                    "",  # ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹ï¼ˆä»Šå›ã¯ç©ºï¼‰
                    job.get('job_description', '')
                ]
                data_rows.append(row)
            
            # ãƒãƒƒãƒã§æ›¸ãè¾¼ã¿
            if data_rows:
                worksheet.append_rows(data_rows)
                
                # åˆ—å¹…ã®èª¿æ•´ï¼ˆå¯èƒ½ãªå ´åˆï¼‰
                try:
                    worksheet.update_column_properties('A', {"pixelSize": 200})  # æ–½è¨­å
                    worksheet.update_column_properties('B', {"pixelSize": 150})  # ä»£è¡¨è€…å
                    worksheet.update_column_properties('C', {"pixelSize": 300})  # å‹¤å‹™åœ°
                    worksheet.update_column_properties('D', {"pixelSize": 250})  # æ±‚äººURL
                    worksheet.update_column_properties('E', {"pixelSize": 150})  # é›»è©±ç•ªå·
                    worksheet.update_column_properties('F', {"pixelSize": 150})  # ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹
                    worksheet.update_column_properties('G', {"pixelSize": 500})  # æ¥­å‹™å†…å®¹
                except Exception:
                    pass  # åˆ—å¹…è¨­å®šã«å¤±æ•—ã—ã¦ã‚‚ç¶šè¡Œ
                
                # ãƒ˜ãƒƒãƒ€ãƒ¼è¡Œã®ã‚¹ã‚¿ã‚¤ãƒ«è¨­å®šï¼ˆå¯èƒ½ãªå ´åˆï¼‰
                try:
                    header_format = {
                        "backgroundColor": {"red": 0.8, "green": 0.8, "blue": 0.8},
                        "horizontalAlignment": "CENTER",
                        "textFormat": {"bold": True}
                    }
                    worksheet.format('A1:G1', header_format)
                except Exception:
                    pass  # ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆè¨­å®šã«å¤±æ•—ã—ã¦ã‚‚ç¶šè¡Œ
            
            return True, f"ãƒ‡ãƒ¼ã‚¿ã‚’ '{sheet_name}' ã‚·ãƒ¼ãƒˆã«æ­£å¸¸ã«ä¿å­˜ã—ã¾ã—ãŸã€‚"
        
        except Exception as e:
            return False, f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}"