### 1. プロジェクト概要

| 項目 | 内容 |
| :--- | :--- |
| **1.1. ツール名称** | 多目的 求人サイトスクレイピング・ダッシュボード |
| **1.2. 目的** | 複数の求人サイトから、それぞれのサイトに最適化されたUIを通じて求人情報を効率的に収集し、データとして活用可能にすること。 |
| **1.3. ツールコンセプト** | サイドバーで対象サイトを選択すると、メイン画面のUIがそのサイト専用の検索画面に切り替わるダッシュボード形式。 |
| **1.4. 開発フレームワーク**| Python, Streamlit |
| **1.5. 開発スコープ**| **フェーズ1**: 「メディカル・コンシェルジュネット」のUIとスクレイピング機能を完璧に実装する。<br>**フェーズ2**: 「美容ナース.com」を追加し、複数サイト対応の基盤を確立する。<br>**フェーズ3**: 「とらばーゆ」を追加し、詳細情報抽出機能とGoogle Sheetsエクスポート機能を実装する。 |

---
### 2. 機能要件

#### 2.1. 全体UI構造（ダッシュボード共通）
どのサイトを選択しても共通となる、アプリケーションの基本的な骨格に関する要件。

| No. | 機能名 | 機能概要 |
|:---:|:---|:---|
| 2.1.1 | **サイト選択サイドバー** | 画面左側のサイドバーに、対象となる求人サイトのリストをラジオボタン形式で表示する。ユーザーはここから操作対象のサイトを選択する。 |
| 2.1.2 | **動的UI表示エリア** | メイン画面のエリア。サイドバーでのサイト選択に応じて、後述の「2.2. サイト別UI定義」で定められた専用UIに内容が完全に切り替わる。 |
| 2.1.3 | **結果表示エリア** | スクレイピング実行後、メイン画面の下部に取得結果を表示する共通エリア。以下の要素を含む。<br>・**取得件数:** `st.success`等で「〇〇件の求人が見つかりました。」と表示。<br>・**結果データテーブル:** `st.dataframe`で取得データを表示。<br>・**CSVダウンロードボタン:** `st.download_button`で結果をCSV出力。<br>・**Google Sheetsエクスポート機能**（とらばーゆのみ） |

#### 2.2. サイト別UI定義
サイドバーの選択に応じて、メイン画面の「動的UI表示エリア」に表示される、各サイト専用のインターフェース定義。

| No. | 対象サイト | UI仕様 |
|:---:|:---|:---|
| **2.2.1** | **メディカル・コンシェルジュネット** | **以下のコンポーネントを上から順に配置する。**<br><br>**■ 見出し**<br>・大見出しとして「`メディカル・コンシェルジュネット：検索条件`」を表示する。<br><br>**■ 検索条件入力部**<br>・2カラムレイアウトで入力項目を配置する。<br>  - **左カラム:**<br>    1. `職種`：プルダウン (`st.selectbox`)<br>    2. `働き方`：プルダウン (`st.selectbox`)<br>  - **右カラム:**<br>    1. `都道府県`：プルダウン (`st.selectbox`)<br>    2. `施設区分`：プルダウン (`st.selectbox`)<br><br>**■ フリーワード入力部**<br>・上記カラムの下に「`フリーワード`」のラベルを持つ一行テキストボックス (`st.text_input`) を配置する。<br><br>**■ 実行部**<br>・最下部に「`この条件で検索する`」というラベルのボタン (`st.button`) を配置する。<br><br>*(補足: 各プルダウンの選択肢は、対象サイトのHTMLから動的に取得したものを表示する)*|
| **2.2.2** | **美容ナース.com** | **以下のコンポーネントを上から順に配置する。**<br><br>**■ 見出し**<br>・大見出しとして「`美容ナース.com：検索条件`」を表示する。<br><br>**■ 検索条件入力部**<br>・3カラムレイアウトで入力項目を配置する。<br>  - **左カラム (エリア):**<br>    1. `都道府県`：プルダウン (`st.selectbox`)<br>  - **中央カラム (給料):**<br>    1. `給料を選択`：プルダウン (`st.selectbox`)<br>  - **右カラム (ご希望の診療科目):**<br>    1. `美容外科`：チェックボックス (`st.checkbox`)<br>    2. `美容皮膚科`：チェックボックス (`st.checkbox`)<br><br>**■ クリニック名入力部**<br>・上記カラムの下に「`クリニック名`」のラベルを持つ一行テキストボックス (`st.text_input`) を配置する。<br><br>**■ 実行部**<br>・最下部に「`求人を探す`」というラベルのボタン (`st.button`) を配置する。<br><br>*(補足: 都道府県と給料の選択肢は、対象サイトのHTMLから動的に取得したものを表示する)*|
| **2.2.3** | **とらばーゆ** | **以下のコンポーネントを上から順に配置する。**<br><br>**■ 見出し**<br>・大見出しとして「`とらばーゆ 求人情報スクレイピングツール`」を表示する。<br><br>**■ 検索条件入力部**<br>・職種名や施設名を入力するテキストボックス (`st.text_input`) を配置する。<br>・サイドバーに取得する求人数を設定するスライダー (`st.slider`) を配置する（1〜300件）。<br><br>**■ 実行部**<br>・「`開始`」というラベルのボタン (`st.button`) を配置する。<br><br>**■ 結果表示**<br>・プログレスバーで進捗状況を表示<br>・取得完了後に詳細情報を表形式で表示<br>・Google Sheetsエクスポート機能を提供|

---
### 3. データ要件（抽出項目）

#### 3.1. 基本項目（全サイト共通）
| No. | 項目名 | データ型 | 備考 |
|:---:|:---|:---|:---|
| 3.1.1 | **求人No** | 文字列 | 案件を特定する一意のID。 |
| 3.1.2 | **タイトル** | 文字列 | 求人案件のタイトル。 |
| 3.1.3 | **職種** | 文字列 | |
| 3.1.4 | **勤務地** | 文字列 | |
| 3.1.5 | **施設** | 文字列 | |
| 3.1.6 | **業務 (働き方)** | 文字列 | |
| 3.1.7 | **給与** | 文字列 | 「(続く)」の部分も含め、取得できるテキストをそのまま格納する。 |
| 3.1.8 | **交通** | 文字列 | |
| 3.1.9 | **詳細ページURL** | 文字列 | |
| 3.1.10 | **情報源サイト名** | 文字列 | どのサイトからの情報か識別するため。「メディカル・コンシェルジュネット」「美容ナース.com」「とらばーゆ」等のサイト名を格納。 |

#### 3.2. とらばーゆ専用拡張項目
| No. | 項目名 | データ型 | 備考 |
|:---:|:---|:---|:---|
| 3.2.1 | **施設名** | 文字列 | 求人を出している施設・企業の正式名称 |
| 3.2.2 | **代表者名** | 文字列 | 施設の代表者・院長・理事長等の名前 |
| 3.2.3 | **電話番号** | 文字列 | 施設の連絡先電話番号（フリーダイヤル含む） |
| 3.2.4 | **メールアドレス** | 文字列 | 施設の連絡先メールアドレス（現在未実装） |
| 3.2.5 | **業務内容** | 文字列 | 詳細な仕事内容・職務内容の説明 |

---
### 4. 非機能要件
システムの品質や制約に関する要件。

| No. | 要件名 | 内容 |
|:---:|:---|:---|
| 4.1 | **パフォーマンス** | 検索条件確定後、ユーザーが待てる現実的な時間内（数分以内を目標）に結果を表示する。 |
| 4.2 | **信頼性・堅牢性** | - **サーバーへの配慮**: サイトに過度な負荷をかけないよう、リクエストごとに1〜3秒の待機時間 (`time.sleep`) を設ける。<br>- **エラーハンドリング**: ネットワークエラーやサイトの構造変更で情報が取得できない場合、処理を安全に停止し、エラーメッセージをユーザーに通知する。<br>- **リトライ機能**: とらばーゆでは最大5回のリトライ機能を実装し、一時的なネットワークエラーに対応する。 |
| 4.3 | **保守性** | - **コードの可読性**: 関数や変数の命名を分かりやすくし、コメントを適切に記述する。<br>- **設定の分離**: URLやCSSセレクタなどの変更されやすい値は、コードの先頭部分で定数として管理し、変更を容易にする。 |
| 4.4 | **データ整合性** | - **Google Sheetsエクスポート**: とらばーゆから取得したデータを統一フォーマットでGoogle Sheetsに出力する機能を提供する。<br>- **データクリーニング**: 取得したデータの不要な文字列除去や整形を自動的に実行する。 |

---
### 5. 開発アーキテクチャ方針

| 項目 | 方針 |
|:--- |:---|
| **5.1. モジュール性** | サイトごとのロジックを完全に分離する。<br>・`app.py`：UIの切り替え制御と全体レイアウトのみ担当。<br>・`scrapers/`フォルダ：サイト別の処理を格納。<br>・`scrapers/medical_concierge.py`：**UI生成とスクレイピング実行の両方を担う**、メディカルコンシェルジュ専用モジュール。<br>・`scrapers/biyou_nurse.py`：**UI生成とスクレイピング実行の両方を担う**、美容ナース.com専用モジュール。<br>・`scrapers/torabayu_scraper.py`：**UI生成とスクレイピング実行の両方を担う**、とらばーゆ専用モジュール。 |
| **5.2. 開発フェーズ** | **フェーズ1**: 基盤（`app.py`の枠組み）と`medical_concierge.py`の実装に集中する。<br>**フェーズ2**: 「美容ナース.com」用の`scrapers/biyou_nurse.py`を追加開発し、複数サイト対応の基盤を確立する。<br>**フェーズ3**: 「とらばーゆ」用の`scrapers/torabayu_scraper.py`を追加開発し、詳細情報抽出機能とGoogle Sheetsエクスポート機能を実装する。 |
| **5.3. スクレイピング技術**| **メディカル・コンシェルジュネット**:<br>- **対象サイトURL**: `https://www.concier.net/jobs/search/`<br>- **文字コード**: `shift_jis`に対応<br>- **検索方式**: HTTP POSTリクエストを再現<br>- **ページネーション**: `requests.Session`オブジェクトを利用してセッションを維持し、次ページURLへGETリクエスト<br><br>**美容ナース.com**:<br>- **対象サイトURL**: `https://biyou-nurse.com/search/`<br>- **文字コード**: `UTF-8`<br>- **検索方式**: HTTP GETリクエスト<br>- **ページネーション**: JavaScript制御のため、URLパラメータ (`&page=2`など) でのアクセスを試行<br><br>**とらばーゆ**:<br>- **対象サイトURL**: `https://toranet.jp/prefectures/tokyo/job_search/kw/{encoded_keyword}`<br>- **文字コード**: `UTF-8`<br>- **検索方式**: HTTP GETリクエスト<br>- **ページネーション**: `/page/{page_number}` パラメータでのページ遷移<br>- **詳細情報取得**: 各求人の詳細ページから施設名、代表者、電話番号等を抽出 |
| **5.4. 使用ライブラリ** | - `streamlit`: UI構築<br>- `requests`: HTTP通信<br>- `beautifulsoup4`: HTML解析<br>- `pandas`: データ整形・CSV出力<br>- `lxml`: `beautifulsoup4`用の高速パーサー<br>- `gspread`: Google Sheets連携（とらばーゆ専用）<br>- `google-auth`: Google API認証（とらばーゆ専用） |

################################################################################
#
# 実装における技術的な注意点
#
################################################################################

## メディカル・コンシェルジュネットにおける注意点

### 1. 検索リクエストの再現（最重要）

-   **方式**: `<form action="/jobs/search" method="post">` より、**HTTP POSTリクエスト**を使用する。
-   **送信先URL**: `https://www.concier.net/jobs/search`
-   **送信データ（ペイロード）**: UIで選択された条件を、`<input>`や`<select>`の`name`属性と`value`属性のペアからなる辞書として作成し、`requests.post()`の`data`引数に渡す。
    -   例: `payload = {'jobId': 'oth_010121', 'localId': '12', ...}`

### 2. 文字コードの取り扱い

-   **指定**: `<meta ... charset=shift_jis" />` より、文字コードは **Shift_JIS**。
-   **対策**: `requests`で取得した`response`オブジェクトに対し、`response.encoding = 'shift_jis'` または `response.encoding = response.apparent_encoding` を設定してから `.text` プロパティにアクセスし、文字化けを防ぐ。

### 3. ページネーション（複数ページの巡回）

-   **仕組み**: このサイトはPOST検索の結果をサーバー側セッションで保持している。
-   **対策**: `requests.Session()` を使用する。
    1.  `session = requests.Session()` でセッションを開始する。
    2.  初回の検索は `session.post()` で実行する。
    3.  検索結果ページから次ページへのリンク（例: `/jobs/search/.../page~2`）を抽出し、`session.get()` でアクセスする。これによりCookieが引き継がれ、検索状態が維持される。
    4.  次ページへのリンクがなくなるまで、3の処理をループする。

### 4. データ抽出のためのCSSセレクタ戦略

-   **全案件ブロック**: `div.job-dtl-itm`
-   **項目別抽出 (各ブロック内)**:
    -   **求人No**: `div.job-dtl-no > p`
    -   **タイトル**: `div.job-dtl-ttl > h3`
    -   **詳細ページURL**: `div.job-dtl-btn > a.btn` (最初のaタグ) の `href` 属性。ベースURLとの結合を忘れないこと。
    -   **テーブルデータ**: `table.job-dtl-cont tr` で各行を取得後、行内の`th` (項目名)と`td` (値)を紐づけて辞書に格納する。項目の順序変動に強いロジックを組む。

### 5. 堅牢性の確保（エラーハンドリング）

-   **`try-except`構文**:
    -   個々のデータ抽出処理（`find`や`select`）を`try-except AttributeError`で囲み、特定のデータが存在しなくてもプログラムが停止しないようにする。見つからない場合は `None` や空文字を代入する。
    -   HTTPリクエスト（`session.post/get`）を`try-except requests.exceptions.RequestException`で囲み、ネットワークエラーを捕捉する。
-   **User-Agentの設定**: リクエストヘッダーにブラウザを偽装する`User-Agent`を設定し、ボット判定を回避する。
    -   `headers = {'User-Agent': 'Mozilla/5.0 ...'}`

### 6. サーバーへの配慮

-   **待機時間**: ループ処理内でページを取得するごとに `time.sleep(2)` のように2秒程度の待機時間を設ける。短時間での連続アクセスは絶対に避けること。

## 美容ナース.comにおける注意点

### 1. 検索リクエストの再現

-   **方式**: **HTTP GETリクエスト**を使用する。
-   **送信先URL**: `https://biyou-nurse.com/search/`
-   **パラメータ**: URLパラメータとして検索条件を付与する。
    -   例: `https://biyou-nurse.com/search/?prefecture=tokyo&salary=high&biyou_geka=1`

### 2. 文字コードの取り扱い

-   **指定**: 文字コードは **UTF-8**。
-   **対策**: `requests`のデフォルトエンコーディングで対応可能。

### 3. ページネーション（複数ページの巡回）

-   **仕組み**: JavaScript制御によるページネーション。
-   **対策**: URLパラメータ (`&page=2`, `&page=3`など) を追加してアクセスを試行する。
    1.  初回検索でベースURLにパラメータを付与してGETリクエスト。
    2.  次ページは `&page=2` を追加したURLで再度GETリクエスト。
    3.  404エラーまたは結果が0件になるまでページ番号を増やして継続。

### 4. データ抽出のためのCSSセレクタ戦略

-   **全案件ブロック**: `ul.list_jobs2 > li`
-   **項目別抽出 (各ブロック内)**:
    -   **タイトル**: `h2`タグ
    -   **クリニック名**: `p.clinick_name`
    -   **詳細ページURL**: `a`タグの`href`属性 (ベースURLとの結合が必要)
    -   **勤務地**: `span.cate`タグ
    -   **メインの施術**: `dl > dd`タグ

### 5. 堅牢性の確保（エラーハンドリング）

-   **`try-except`構文**: メディカル・コンシェルジュネットと同様の方針。
-   **User-Agentの設定**: 同様にブラウザを偽装する。

### 6. サーバーへの配慮

-   **待機時間**: ページ遷移ごとに `time.sleep(2)` で2秒以上の待機時間を設ける。

## とらばーゆにおける注意点

### 1. 検索リクエストの再現

-   **方式**: **HTTP GETリクエスト**を使用する。
-   **送信先URL**: `https://toranet.jp/prefectures/tokyo/job_search/kw/{encoded_keyword}`
-   **エンコーディング**: キーワードは `urllib.parse.quote()` でURLエンコードする。

### 2. 文字コードの取り扱い

-   **指定**: 文字コードは **UTF-8**。
-   **対策**: `requests`のデフォルトエンコーディングで対応可能。

### 3. ページネーション（複数ページの巡回）

-   **仕組み**: URL構造によるページネーション。
-   **対策**: `/page/{page_number}` を追加してアクセスする。
    1.  初回検索でベースURLにアクセス。
    2.  次ページは `/page/2`, `/page/3` を追加したURLで再度GETリクエスト。
    3.  求人リンクが取得できなくなるまで継続。

### 4. データ抽出のためのCSSセレクタ戦略

-   **求人リンク取得**: `a[href]` タグから求人詳細ページのURLを抽出
-   **詳細ページでの項目別抽出**:
    -   **施設名**: `div.corpNameWrap > span`, `h1`, `h2` 等の複数セレクタで試行
    -   **代表者**: `p.styles_content__HWIR6` で前要素が「代表者」のh3タグを持つものを検索
    -   **勤務地**: `p.styles_content__HWIR6` で前要素が「勤務地」のh3タグを持つものを検索
    -   **電話番号**: `p.styles_content__HWIR6` で前要素が「代表電話番号」のh3タグを持つものを検索
    -   **業務内容**: `th` タグが「職種/仕事内容」を含む場合の隣接 `td` タグ

### 5. 堅牢性の確保（エラーハンドリング）

-   **リトライ機能**: 最大5回のリトライ機能を実装し、一時的なネットワークエラーに対応。
-   **プログレスバー**: `st.progress()` で進捗状況をリアルタイム表示。
-   **エラー制限**: エラー件数が全体の半分または50件に達した場合は処理を中断。

### 6. データクリーニング機能

-   **施設名クリーニング**: 「の求人詳細」「とらばーゆ」等の不要文字列を除去。
-   **代表者名クリーニング**: 住所情報や電話番号が混在している場合の分離処理。
-   **電話番号整形**: フリーダイヤル（0120）や市外局番の自動フォーマット。

### 7. Google Sheetsエクスポート機能

-   **認証**: `credentials/service_account.json` ファイルによるサービスアカウント認証。
-   **データ形式**: 7項目統一フォーマット（施設名、代表者名、勤務地、求人URL、電話番号、メールアドレス、業務内容）。
-   **シート管理**: 既存シートへの追記または新規シート作成の選択可能。

### 8. サーバーへの配慮

-   **待機時間**: ページ遷移ごとに 1〜3秒、詳細ページアクセスごとに 0.3〜1秒の待機時間を設定。
-   **User-Agent**: ブラウザを偽装したヘッダーを設定してボット判定を回避。
