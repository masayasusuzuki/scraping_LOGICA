#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import sys
import os
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from scrapers.kyujinbox_scraper import KyujinboxScraper
import json

def test_kyujinbox_scraper():
    """ä¿®æ­£ã•ã‚ŒãŸæ±‚äººãƒœãƒƒã‚¯ã‚¹ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ã‚’ãƒ†ã‚¹ãƒˆ"""
    
    print("=== æ±‚äººãƒœãƒƒã‚¯ã‚¹ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ä¿®æ­£ç‰ˆãƒ†ã‚¹ãƒˆ ===\n")
    
    # ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ä½œæˆ
    scraper = KyujinboxScraper()
    
    # ãƒ†ã‚¹ãƒˆæ¡ä»¶
    employment_type_word = "1"  # æ­£ç¤¾å“¡
    keyword = "çœ‹è­·å¸«"
    area = "æ±äº¬éƒ½"
    max_jobs = 3  # ãƒ†ã‚¹ãƒˆç”¨ã«å°‘æ•°
    
    print(f"ãƒ†ã‚¹ãƒˆæ¡ä»¶:")
    print(f"- é›‡ç”¨å½¢æ…‹: {employment_type_word} (æ­£ç¤¾å“¡)")
    print(f"- ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {keyword}")
    print(f"- ã‚¨ãƒªã‚¢: {area}")
    print(f"- æœ€å¤§å–å¾—æ•°: {max_jobs}")
    print()
    
    # ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Ÿè¡Œ
    try:
        results, error = scraper.scrape_jobs(
            employment_type_word=employment_type_word,
            keyword=keyword,
            area=area,
            max_jobs=max_jobs,
            debug_mode=True  # ãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰æœ‰åŠ¹
        )
        
        if error:
            print(f"âŒ ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ: {error}")
            return
        
        if not results:
            print("âŒ çµæœãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")
            return
        
        print(f"\nâœ… æˆåŠŸï¼{len(results)}ä»¶ã®æ±‚äººã‚’å–å¾—ã—ã¾ã—ãŸ\n")
        
        # çµæœã®è©³ç´°è¡¨ç¤º
        for i, job in enumerate(results, 1):
            print(f"--- æ±‚äºº {i} ---")
            print(f"æ–½è¨­å: {job.get('facility_name', 'N/A')}")
            print(f"ä»£è¡¨è€…: {job.get('representative', 'N/A')}")
            print(f"ä½æ‰€: {job.get('address', 'N/A')}")
            print(f"ã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆURL: {job.get('website_url', 'N/A')}")
            print(f"é›»è©±ç•ªå·: {job.get('phone_number', 'N/A')}")
            print(f"ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹: {job.get('email', 'N/A')}")
            print(f"äº‹æ¥­å†…å®¹: {job.get('business_content', 'N/A')}")
            print(f"è©³ç´°ãƒšãƒ¼ã‚¸URL: {job.get('source_url', 'N/A')}")
            print()
        
        # å•é¡Œã®ç¢ºèª
        print("=== ä¿®æ­£çµæœã®ç¢ºèª ===")
        youtube_count = sum(1 for job in results if 'youtube' in job.get('website_url', '').lower())
        valid_website_count = sum(1 for job in results if job.get('website_url') and 'youtube' not in job.get('website_url', '').lower() and job.get('website_url') != 'N/A')
        
        print(f"âœ… ç·å–å¾—ä»¶æ•°: {len(results)}")
        print(f"âŒ YouTubeã®Webã‚µã‚¤ãƒˆURL: {youtube_count}ä»¶")
        print(f"âœ… æœ‰åŠ¹ãªWebã‚µã‚¤ãƒˆURL: {valid_website_count}ä»¶")
        
        if youtube_count == 0:
            print("\nğŸ‰ YouTubeå•é¡ŒãŒè§£æ±ºã•ã‚Œã¾ã—ãŸï¼")
        else:
            print(f"\nâš ï¸  ã¾ã {youtube_count}ä»¶ã®YouTube URLãŒæ®‹ã£ã¦ã„ã¾ã™ã€‚")
        
        # è©³ç´°ãƒšãƒ¼ã‚¸URLã®ç¢ºèª
        jb_url_count = sum(1 for job in results if '/jb/' in job.get('source_url', ''))
        print(f"âœ… /jb/ è©³ç´°ãƒšãƒ¼ã‚¸URL: {jb_url_count}ä»¶")
        
        if jb_url_count == len(results):
            print("ğŸ‰ è©³ç´°ãƒšãƒ¼ã‚¸URLå•é¡Œã‚‚è§£æ±ºã•ã‚Œã¾ã—ãŸï¼")
        else:
            print(f"âš ï¸  /jb/ URLã§ãªã„è©³ç´°ãƒšãƒ¼ã‚¸ãŒ{len(results) - jb_url_count}ä»¶ã‚ã‚Šã¾ã™ã€‚")
        
        # JSONãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
        output_file = "kyujinbox_test_results.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        print(f"\nğŸ“ çµæœã‚’ {output_file} ã«ä¿å­˜ã—ã¾ã—ãŸ")
        
    except Exception as e:
        print(f"âŒ äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    test_kyujinbox_scraper() 