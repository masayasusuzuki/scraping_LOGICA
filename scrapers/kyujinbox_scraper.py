import streamlit as st
import requests
from bs4 import BeautifulSoup
import urllib.parse
import time
import random
import html
import re
import pandas as pd
import json

class KyujinboxScraper:
    """æ±‚äººãƒœãƒƒã‚¯ã‚¹ã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼"""
    
    def __init__(self):
        self.base_url = "https://xn--pckua2a7gp15o89zb.com/"
        self.session = requests.Session()
        self.session.headers.update(self.get_headers())
        
    def get_headers(self):
        """ãƒªã‚¯ã‚¨ã‚¹ãƒˆãƒ˜ãƒƒãƒ€ãƒ¼ã‚’ç”Ÿæˆ"""
        return {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',
            'Accept-Language': 'ja,en-US;q=0.9,en;q=0.8',
            'Referer': 'https://xn--pckua2a7gp15o89zb.com/',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Cache-Control': 'max-age=0',
        }
    
    def get_employment_types(self):
        """æ±‚äººãƒœãƒƒã‚¯ã‚¹ãƒˆãƒƒãƒ—ãƒšãƒ¼ã‚¸ã‹ã‚‰é›‡ç”¨å½¢æ…‹ã®é¸æŠè‚¢ã‚’å–å¾—"""
        try:
            response = self.session.get(self.base_url, timeout=30)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            select_element = soup.find('select', {'name': 'employType'})
            
            if not select_element:
                # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®é¸æŠè‚¢ã‚’è¿”ã™
                return [
                    {"text": "æ­£ç¤¾å“¡", "data_search_word": "æ­£ç¤¾å“¡"},
                    {"text": "ã‚¢ãƒ«ãƒã‚¤ãƒˆãƒ»ãƒ‘ãƒ¼ãƒˆ", "data_search_word": "ãƒã‚¤ãƒˆ"},
                    {"text": "æ´¾é£ç¤¾å“¡", "data_search_word": "æ´¾é£"},
                    {"text": "å¥‘ç´„ç¤¾å“¡", "data_search_word": "å¥‘ç´„"},
                ]
            
            options = []
            for option in select_element.find_all('option'):
                if option.get('value') and option.get('data-search_word'):
                    options.append({
                        "text": option.text.strip(),
                        "data_search_word": option.get('data-search_word')
                    })
            
            return options if options else [
                {"text": "æ­£ç¤¾å“¡", "data_search_word": "æ­£ç¤¾å“¡"},
                {"text": "ã‚¢ãƒ«ãƒã‚¤ãƒˆãƒ»ãƒ‘ãƒ¼ãƒˆ", "data_search_word": "ãƒã‚¤ãƒˆ"},
            ]
            
        except Exception as e:
            # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸå ´åˆã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®é¸æŠè‚¢ã‚’è¿”ã™
            return [
                {"text": "æ­£ç¤¾å“¡", "data_search_word": "æ­£ç¤¾å“¡"},
                {"text": "ã‚¢ãƒ«ãƒã‚¤ãƒˆãƒ»ãƒ‘ãƒ¼ãƒˆ", "data_search_word": "ãƒã‚¤ãƒˆ"},
                {"text": "æ´¾é£ç¤¾å“¡", "data_search_word": "æ´¾é£"},
                {"text": "å¥‘ç´„ç¤¾å“¡", "data_search_word": "å¥‘ç´„"},
            ]
    
    def generate_search_url(self, employment_type_word, keyword, area):
        """æ¤œç´¢URLã‚’ç”Ÿæˆ"""
        try:
            # æ±‚äººãƒœãƒƒã‚¯ã‚¹ã®å®Ÿéš›ã®URLå½¢å¼ã«åˆã‚ã›ã‚‹
            # ä¾‹: https://xn--pckua2a7gp15o89zb.com/çœ‹è­·å¸«ã®ä»•äº‹-æ±äº¬éƒ½?e=1
            
            # æ¤œç´¢ã‚¯ã‚¨ãƒªã‚’æ§‹ç¯‰
            url_parts = []
            
            if keyword:
                url_parts.append(f"{keyword}ã®ä»•äº‹")
            
            if area:
                # éƒ½é“åºœçœŒåã‚’æ­£è¦åŒ–
                area_normalized = area
                if not area.endswith(('éƒ½', 'é“', 'åºœ', 'çœŒ')):
                    # éƒ½é“åºœçœŒåã§ãªã„å ´åˆã¯ã€ä¸€èˆ¬çš„ãªåœ°åŸŸã¨ã—ã¦æ‰±ã†
                    if area in ['æ±äº¬', 'å¤§é˜ª', 'äº¬éƒ½']:
                        area_normalized = area + ('éƒ½' if area == 'æ±äº¬' else 'åºœ')
                    elif area == 'åŒ—æµ·é“':
                        area_normalized = area
                    else:
                        # ãã®ä»–ã®å ´åˆã¯ãã®ã¾ã¾ä½¿ç”¨
                        area_normalized = area
                url_parts.append(area_normalized)
            
            if not url_parts:
                # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚‚ã‚¨ãƒªã‚¢ã‚‚ãªã„å ´åˆã¯ã€é›‡ç”¨å½¢æ…‹ã®ã¿
                if employment_type_word:
                    url_parts.append(f"{employment_type_word}ã®ä»•äº‹")
                else:
                    return self.base_url
            
            # URLä½œæˆ
            path_part = "-".join(url_parts)
            encoded_path = urllib.parse.quote(path_part, safe='-')
            
            # é›‡ç”¨å½¢æ…‹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’è¿½åŠ 
            params = []
            if employment_type_word:
                employment_map = {
                    "æ­£ç¤¾å“¡": "1",
                    "ãƒã‚¤ãƒˆ": "2", 
                    "æ´¾é£": "3",
                    "å¥‘ç´„": "4"
                }
                emp_id = employment_map.get(employment_type_word, "")
                if emp_id:
                    params.append(f"e={emp_id}")
            
            # æœ€çµ‚URLæ§‹ç¯‰
            search_url = f"{self.base_url}{encoded_path}"
            if params:
                search_url += f"?{'&'.join(params)}"
            
            return search_url
            
        except Exception as e:
            return None
    
    def make_request(self, url, max_retries=3, timeout=30):
        """ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’é€ä¿¡ï¼ˆãƒªãƒˆãƒ©ã‚¤æ©Ÿèƒ½ä»˜ãï¼‰"""
        for attempt in range(max_retries):
            try:
                if attempt > 0:
                    sleep_time = random.uniform(2, 4)
                    time.sleep(sleep_time)
                else:
                    time.sleep(random.uniform(0.5, 1.5))
                
                response = self.session.get(url, timeout=timeout)
                response.raise_for_status()
                return response, None
                
            except requests.exceptions.HTTPError as e:
                if e.response.status_code == 503 and attempt < max_retries - 1:
                    continue
                return None, f"HTTPã‚¨ãƒ©ãƒ¼: {e.response.status_code} - {e}"
            except requests.exceptions.Timeout:
                if attempt < max_retries - 1:
                    continue
                return None, "ãƒªã‚¯ã‚¨ã‚¹ãƒˆãŒã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã—ã¾ã—ãŸ"
            except requests.exceptions.RequestException as e:
                if attempt < max_retries - 1:
                    continue
                return None, f"ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {str(e)}"
        
        return None, "æœ€å¤§å†è©¦è¡Œå›æ•°ã«é”ã—ã¾ã—ãŸ"
    
    def extract_phone_number(self, text):
        """ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰é›»è©±ç•ªå·ã‚’æŠ½å‡º"""
        if not text:
            return None
        
        # é›»è©±ç•ªå·ã®ãƒ‘ã‚¿ãƒ¼ãƒ³å®šç¾©
        phone_patterns = [
            # ãƒ•ãƒªãƒ¼ãƒ€ã‚¤ãƒ¤ãƒ«ãƒ»ç‰¹ç•ª
            r'0120[-\s]?\d{3}[-\s]?\d{3}',
            r'0800[-\s]?\d{3}[-\s]?\d{4}',
            r'0570[-\s]?\d{3}[-\s]?\d{3}',
            r'050[-\s]?\d{4}[-\s]?\d{4}',
            
            # æºå¸¯é›»è©±
            r'0[987]0[-\s]?\d{4}[-\s]?\d{4}',
            
            # å›ºå®šé›»è©±
            r'0[3-6][-\s]?\d{4}[-\s]?\d{4}',
            r'0[1-9]\d[-\s]?\d{3,4}[-\s]?\d{4}',
            r'0[1-9]\d{2}[-\s]?\d{2,3}[-\s]?\d{4}',
            
            # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ä»˜ããƒ‘ã‚¿ãƒ¼ãƒ³
            r'é›»è©±[ï¼š:\s]*([0-9ï¼-ï¼™][0-9ï¼-ï¼™\-\(\)\s]{8,13})',
            r'TEL[ï¼š:\s]*([0-9ï¼-ï¼™][0-9ï¼-ï¼™\-\(\)\s]{8,13})',
        ]
        
        for pattern in phone_patterns:
            matches = re.search(pattern, text, re.IGNORECASE)
            if matches:
                phone = matches.group(1) if len(matches.groups()) > 0 else matches.group(0)
                
                # æ•°å­—ã®ã¿æŠ½å‡ºã—ã¦æ¤œè¨¼
                digits_only = re.sub(r'[^\d]', '', phone)
                
                # é›»è©±ç•ªå·ã¨ã—ã¦æœ‰åŠ¹ãªæ¡æ•°ã‹ãƒã‚§ãƒƒã‚¯ï¼ˆ10æ¡ã¾ãŸã¯11æ¡ï¼‰
                if len(digits_only) not in [10, 11]:
                    continue
                
                # 0ã§å§‹ã¾ã‚‹ã“ã¨ã‚’ç¢ºèª
                if not digits_only.startswith('0'):
                    continue
                
                # ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆèª¿æ•´
                if digits_only.startswith('0120') and len(digits_only) == 10:
                    return f"{digits_only[:4]}-{digits_only[4:7]}-{digits_only[7:]}"
                elif len(digits_only) == 11:
                    return f"{digits_only[:3]}-{digits_only[3:7]}-{digits_only[7:]}"
                elif len(digits_only) == 10:
                    if digits_only.startswith('03') or digits_only.startswith('06'):
                        return f"{digits_only[:2]}-{digits_only[2:6]}-{digits_only[6:]}"
                    else:
                        return f"{digits_only[:3]}-{digits_only[3:6]}-{digits_only[6:]}"
                
                return phone
        
        return None
    
    def extract_representative(self, text):
        """ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ä»£è¡¨è€…åã‚’æŠ½å‡º"""
        if not text:
            return ""
        
        text = re.sub(r'<[^>]+>', ' ', text)
        text = re.sub(r'[\[\]ã€ã€‘ï¼»ï¼½()ï¼ˆï¼‰ã€Œã€ã€ã€â‰ªâ‰«<>ï¼œï¼""\'\']+', ' ', text)
        text = re.sub(r'\s+', ' ', text).strip()
        
        patterns = [
            r'ä»£è¡¨è€…\s*[\n\r:ï¼š]*\s*([^\n\rï¼ˆ(ã€ï¼»[{]+)',
            r'ä»£è¡¨å–ç· å½¹\s*[\n\r:ï¼š]*\s*([^\n\rï¼ˆ(ã€ï¼»[{]+)',
            r'é™¢é•·\s*[\n\r:ï¼š]*\s*([^\n\r:ï¼šï¼ˆ(ã€ï¼»[{]+)',
            r'ç†äº‹é•·\s*[\n\r:ï¼š]*\s*([^\n\r:ï¼šï¼ˆ(ã€ï¼»[{]+)',
        ]
        
        for pattern in patterns:
            matches = re.search(pattern, text, re.DOTALL)
            if matches and matches.group(1):
                name = matches.group(1).strip()
                if name and name != "è€…" and len(name) > 1:
                    name = re.sub(r'æ‰€åœ¨ä½æ‰€.*$', '', name)
                    name = re.sub(r'ä½æ‰€.*$', '', name)
                    name = re.sub(r'[0-9ï¼-ï¼™]{5,}.*$', '', name)
                    name = name.strip()
                    if len(name) > 1:
                        return name
        
        return ""
    
    def search_contact_info_web(self, facility_name, debug_mode=False):
        """Webæ¤œç´¢ã§ä¼æ¥­ã®é€£çµ¡å…ˆæƒ…å ±ã‚’å–å¾—ï¼ˆå¼·åŒ–ç‰ˆï¼‰"""
        contact_info = {
            "phone": "",
            "email": "",
            "representative": ""
        }
        
        try:
            if debug_mode:
                st.info(f"ğŸ” Webæ¤œç´¢é–‹å§‹: {facility_name}")
            
            # è¤‡æ•°ã®æ¤œç´¢ã‚¯ã‚¨ãƒªãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’è©¦è¡Œ
            search_patterns = [
                f"{facility_name} é›»è©±ç•ªå· TEL",
                f"{facility_name} é€£çµ¡å…ˆ ãŠå•ã„åˆã‚ã›",
                f"{facility_name} ä¼šç¤¾æ¦‚è¦ é›»è©±",
                f"{facility_name} ä»£è¡¨ TEL é›»è©±"
            ]
            
            best_phone = ""
            best_email = ""
            best_representative = ""
            
            for idx, search_query in enumerate(search_patterns):
                if debug_mode:
                    st.info(f"ğŸ” æ¤œç´¢ãƒ‘ã‚¿ãƒ¼ãƒ³ {idx+1}: {search_query}")
                
                # Webæ¤œç´¢ã‚’å®Ÿè¡Œ
                result_text = self.perform_web_search(search_query)
                
                if result_text:
                    if debug_mode:
                        st.code(f"æ¤œç´¢çµæœï¼ˆæœ€åˆã®300æ–‡å­—ï¼‰: {result_text[:300]}...")
                    
                    # é›»è©±ç•ªå·ã‚’æŠ½å‡ºï¼ˆè¤‡æ•°ãƒ‘ã‚¿ãƒ¼ãƒ³å¯¾å¿œï¼‰
                    phone = self.extract_phone_number_enhanced(result_text, facility_name, debug_mode)
                    if phone and not best_phone:
                        best_phone = phone
                        if debug_mode:
                            st.success(f"âœ… é›»è©±ç•ªå·ç™ºè¦‹: {phone}")
                    
                    # ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹æŠ½å‡º
                    email = self.extract_email_enhanced(result_text, facility_name, debug_mode)
                    if email and not best_email:
                        best_email = email
                        if debug_mode:
                            st.success(f"âœ… ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹ç™ºè¦‹: {email}")
                    
                    # ä»£è¡¨è€…åæŠ½å‡º
                    representative = self.extract_representative_enhanced(result_text, facility_name, debug_mode)
                    if representative and not best_representative:
                        best_representative = representative
                        if debug_mode:
                            st.success(f"âœ… ä»£è¡¨è€…åç™ºè¦‹: {representative}")
                    
                    # ååˆ†ãªæƒ…å ±ãŒé›†ã¾ã£ãŸã‚‰æ—©æœŸçµ‚äº†
                    if best_phone and best_email:
                        break
                
                # ãƒ¬ãƒ¼ãƒˆåˆ¶é™
                time.sleep(1)
            
            contact_info["phone"] = best_phone
            contact_info["email"] = best_email
            contact_info["representative"] = best_representative
            
            if debug_mode:
                st.json({
                    "é›»è©±ç•ªå·": contact_info["phone"] or "è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ",
                    "ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹": contact_info["email"] or "è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ", 
                    "ä»£è¡¨è€…": contact_info["representative"] or "è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ"
                })
            
            return contact_info
            
        except Exception as e:
            if debug_mode:
                st.error(f"ğŸ” Webæ¤œç´¢ã‚¨ãƒ©ãƒ¼: {str(e)}")
            return contact_info
    
    def extract_phone_number_enhanced(self, text, facility_name, debug_mode=False):
        """å¼·åŒ–ã•ã‚ŒãŸé›»è©±ç•ªå·æŠ½å‡º"""
        # è¤‡æ•°ã®é›»è©±ç•ªå·ãƒ‘ã‚¿ãƒ¼ãƒ³
        phone_patterns = [
            # TEL: å½¢å¼
            re.compile(r'(?:TEL|Tel|tel|é›»è©±|â˜)[:ï¼š\s]*(\d{2,4}[-â€\s]?\d{2,4}[-â€\s]?\d{4})'),
            # ãŠå•ã„åˆã‚ã›: å½¢å¼
            re.compile(r'(?:ãŠå•ã„åˆã‚ã›|å•ã„åˆã‚ã›|é€£çµ¡å…ˆ)[:ï¼š\s]*(\d{2,4}[-â€\s]?\d{2,4}[-â€\s]?\d{4})'),
            # ä»£è¡¨é›»è©±: å½¢å¼
            re.compile(r'(?:ä»£è¡¨|æœ¬ç¤¾|å—ä»˜)[:ï¼š\s]*(\d{2,4}[-â€\s]?\d{2,4}[-â€\s]?\d{4})'),
            # ä¸€èˆ¬çš„ãªé›»è©±ç•ªå·ãƒ‘ã‚¿ãƒ¼ãƒ³
            re.compile(r'(\d{2,4}[-â€]\d{2,4}[-â€]\d{4})'),
            # ãƒã‚¤ãƒ•ãƒ³ãªã—ï¼ˆ10-11æ¡ï¼‰
            re.compile(r'(\d{10,11})'),
        ]
        
        best_phone = ""
        
        for pattern in phone_patterns:
            matches = pattern.findall(text)
            for match in matches:
                # é›»è©±ç•ªå·ã®å¦¥å½“æ€§ãƒã‚§ãƒƒã‚¯
                clean_phone = re.sub(r'[-â€\s]', '', match)
                
                # 10-11æ¡ã§ã‚ã‚‹ã“ã¨ã‚’ç¢ºèª
                if 10 <= len(clean_phone) <= 11:
                    # å…ˆé ­ãŒ0ã§å§‹ã¾ã‚‹ã“ã¨ã‚’ç¢ºèªï¼ˆæ—¥æœ¬ã®é›»è©±ç•ªå·ï¼‰
                    if clean_phone.startswith('0'):
                        # ãƒã‚¤ãƒ•ãƒ³ã§æ•´å½¢
                        if len(clean_phone) == 10:
                            formatted_phone = f"{clean_phone[:3]}-{clean_phone[3:6]}-{clean_phone[6:]}"
                        else:  # 11æ¡
                            formatted_phone = f"{clean_phone[:3]}-{clean_phone[3:7]}-{clean_phone[7:]}"
                        
                        if debug_mode:
                            st.info(f"ğŸ“ é›»è©±ç•ªå·å€™è£œ: {formatted_phone}")
                        
                        return formatted_phone
        
        return best_phone
    
    def extract_email_enhanced(self, text, facility_name, debug_mode=False):
        """å¼·åŒ–ã•ã‚ŒãŸãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹æŠ½å‡º"""
        # ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹ãƒ‘ã‚¿ãƒ¼ãƒ³
        email_patterns = [
            # ä¸€èˆ¬çš„ãªãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹
            re.compile(r'([a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,})'),
            # info@å½¢å¼ã‚’å„ªå…ˆ
            re.compile(r'(info@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,})'),
            # contact@å½¢å¼ã‚’å„ªå…ˆ
            re.compile(r'(contact@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,})'),
        ]
        
        # é™¤å¤–ã™ã¹ããƒ‰ãƒ¡ã‚¤ãƒ³ãƒªã‚¹ãƒˆ
        exclude_domains = [
            'google', 'youtube', 'facebook', 'twitter', 'instagram', 
            'linkedin', 'example', 'noreply', 'duckduckgo', 'bing',
            'yahoo', 'gmail', 'hotmail', 'outlook', 'test', 'sample'
        ]
        
        best_email = ""
        
        for pattern in email_patterns:
            matches = pattern.findall(text)
            for email in matches:
                # é™¤å¤–ãƒ‰ãƒ¡ã‚¤ãƒ³ãƒã‚§ãƒƒã‚¯
                if not any(domain in email.lower() for domain in exclude_domains):
                    # ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ç­‰ã‚’é™¤å¤–
                    if not any(ext in email.lower() for ext in ['.png', '.jpg', '.jpeg', '.gif', '.css', '.js']):
                        if debug_mode:
                            st.info(f"ğŸ“§ ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹å€™è£œ: {email}")
                        return email
        
        return best_email
    
    def extract_representative_enhanced(self, text, facility_name, debug_mode=False):
        """å¼·åŒ–ã•ã‚ŒãŸä»£è¡¨è€…åæŠ½å‡º"""
        # ä»£è¡¨è€…åãƒ‘ã‚¿ãƒ¼ãƒ³
        representative_patterns = [
            re.compile(r'(?:ä»£è¡¨å–ç· å½¹|ç¤¾é•·|ä»£è¡¨è€…|CEO|ä¼šé•·|ç†äº‹é•·|é™¢é•·|æ‰€é•·|ä»£è¡¨)[:ï¼š\s]*([^\s\n]{2,10})', re.MULTILINE),
            re.compile(r'([^\s\n]{2,6})\s*(?:ä»£è¡¨å–ç· å½¹|ç¤¾é•·|CEO)', re.MULTILINE),
            re.compile(r'ä»£è¡¨[:ï¼š]\s*([^\s\n]{2,10})', re.MULTILINE),
        ]
        
        for pattern in representative_patterns:
            matches = pattern.findall(text)
            for match in matches:
                # æ—¥æœ¬èªåã‚‰ã—ã„æ–‡å­—åˆ—ã‹ãƒã‚§ãƒƒã‚¯
                if len(match) >= 2 and len(match) <= 8:
                    # æ•°å­—ã‚„è¨˜å·ãŒå«ã¾ã‚Œã¦ã„ãªã„ã‹ãƒã‚§ãƒƒã‚¯
                    if not re.search(r'[0-9\-_@.]', match):
                        if debug_mode:
                            st.info(f"ğŸ‘¤ ä»£è¡¨è€…å€™è£œ: {match}")
                        return match
        
        return ""
    
    def perform_web_search(self, search_term):
        """Webæ¤œç´¢ã‚’å®Ÿè¡Œã™ã‚‹"""
        try:
            import os
            import platform
            
            # ã‚¯ãƒ©ã‚¦ãƒ‰ç’°å¢ƒæ¤œå‡º
            cloud_indicators = [
                'STREAMLIT_CLOUD' in os.environ,
                'STREAMLIT_SHARING_MODE' in os.environ, 
                'HOSTNAME' in os.environ and 'streamlit' in os.environ.get('HOSTNAME', '').lower(),
                platform.node() and 'streamlit' in platform.node().lower(),
                'USER' in os.environ and os.environ.get('USER') == 'appuser',
                'HOME' in os.environ and '/home/appuser' in os.environ.get('HOME', ''),
                'STREAMLIT_SERVER_HEADLESS' in os.environ
            ]
            
            is_cloud_env = any(cloud_indicators)
            
            if is_cloud_env:
                # ã‚¯ãƒ©ã‚¦ãƒ‰ç’°å¢ƒã§ã¯æ¤œç´¢ã‚’ç„¡åŠ¹åŒ–
                return ""
            
            # æ¤œç´¢ã‚¨ãƒ³ã‚¸ãƒ³ã®ãƒªã‚¹ãƒˆ
            search_engines = [
                {
                    'name': 'DuckDuckGo',
                    'url': f"https://html.duckduckgo.com/html/?q={urllib.parse.quote(search_term)}",
                },
                {
                    'name': 'Bing',
                    'url': f"https://www.bing.com/search?q={urllib.parse.quote(search_term)}",
                }
            ]
            
            headers = {
                'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
                'Accept-Language': 'ja,en-US;q=0.5',
                'Accept-Encoding': 'gzip, deflate',
                'DNT': '1',
                'Connection': 'keep-alive',
                'Upgrade-Insecure-Requests': '1'
            }
            
            for engine in search_engines:
                try:
                    response = requests.get(engine['url'], headers=headers, timeout=15)
                    if response.status_code == 200:
                        soup = BeautifulSoup(response.text, 'html.parser')
                        
                        results = []
                        
                        if 'duckduckgo' in engine['url']:
                            # DuckDuckGo HTMLçµæœã®è§£æ
                            result_elements = soup.find_all(['h2', 'span'], class_=lambda x: x and 'result' in str(x))
                            snippet_elements = soup.find_all('a', class_='result__snippet')
                            
                            for elem in result_elements[:3]:
                                text = elem.get_text(strip=True)
                                if text and len(text) > 10:
                                    results.append(text)
                            
                            for elem in snippet_elements[:3]:
                                text = elem.get_text(strip=True)
                                if text and len(text) > 10:
                                    results.append(text)
                        
                        elif 'bing' in engine['url']:
                            # Bingçµæœã®è§£æ
                            result_elements = soup.find_all('h2')
                            snippet_elements = soup.find_all('p')
                            
                            for elem in result_elements[:3]:
                                text = elem.get_text(strip=True)
                                if text and len(text) > 10:
                                    results.append(text)
                            
                            for elem in snippet_elements[:3]:
                                text = elem.get_text(strip=True)
                                if text and len(text) > 20:
                                    results.append(text)
                        
                        if results:
                            return " ".join(results[:5])  # ä¸Šä½5ä»¶ã¾ã§
                    
                    # å„ã‚¨ãƒ³ã‚¸ãƒ³ã®é–“ã«å°‘ã—å¾…æ©Ÿ
                    time.sleep(2)
                    
                except requests.exceptions.RequestException:
                    continue
            
            return ""
            
        except Exception:
            return ""
    
    def extract_jobs_from_page(self, soup, page_url, debug_mode=False):
        """æ¤œç´¢çµæœãƒšãƒ¼ã‚¸ã‹ã‚‰JSONæ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ã¦æ±‚äººæƒ…å ±ã‚’ç›´æ¥æŠ½å‡º"""
        jobs = []
        
        try:
            if debug_mode:
                st.info("ğŸ› DEBUG: æ¤œç´¢çµæœãƒšãƒ¼ã‚¸ã‹ã‚‰JSONæ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡ºä¸­...")
            
            # æ±‚äººãƒœãƒƒã‚¯ã‚¹ã®æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ã‚’å«ã‚€æ±‚äººã‚«ãƒ¼ãƒ‰ã‚’å–å¾—
            job_cards = soup.select('section.p-result_card')
            
            if debug_mode:
                st.info(f"ğŸ› DEBUG: {len(job_cards)} å€‹ã®æ±‚äººã‚«ãƒ¼ãƒ‰ã‚’ç™ºè¦‹")
            
            for idx, card in enumerate(job_cards):
                try:
                    # data-func-show-argå±æ€§ã‚’æŒã¤aã‚¿ã‚°ã‚’æ¢ã™
                    json_link = card.find('a', {'data-func-show-arg': True})
                    
                    if not json_link:
                        if debug_mode and idx < 3:
                            st.warning(f"ğŸ› DEBUG: ã‚«ãƒ¼ãƒ‰ {idx+1} ã§JSONå±æ€§ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                        continue
                    
                    # JSONæ–‡å­—åˆ—ã‚’å–å¾—
                    json_str = json_link.get('data-func-show-arg')
                    
                    if debug_mode and idx < 3:
                        st.info(f"ğŸ› DEBUG: ã‚«ãƒ¼ãƒ‰ {idx+1} ã®JSONæ–‡å­—åˆ—ã‚’å–å¾—")
                        st.code(f"JSONæ–‡å­—åˆ—ï¼ˆæœ€åˆã®200æ–‡å­—ï¼‰: {json_str[:200]}...")
                    
                    # JSONã‚’è§£æï¼ˆ2å±¤æ§‹é€ ï¼‰
                    try:
                        # å¤–å´ã®JSONã‚’ãƒ‘ãƒ¼ã‚¹
                        outer_data = json.loads(json_str)
                        
                        if debug_mode and idx < 3:
                            st.success(f"ğŸ› DEBUG: ã‚«ãƒ¼ãƒ‰ {idx+1} ã®å¤–å´JSONè§£ææˆåŠŸ")
                            st.json(outer_data)
                        
                        # å†…å´ã®jsonãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’å†åº¦ãƒ‘ãƒ¼ã‚¹
                        inner_json_str = outer_data.get('json', '')
                        if not inner_json_str:
                            if debug_mode and idx < 3:
                                st.warning(f"ğŸ› DEBUG: ã‚«ãƒ¼ãƒ‰ {idx+1} å†…å´JSONãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                            continue
                        
                        job_data = json.loads(inner_json_str)
                        
                        if debug_mode and idx < 3:
                            st.success(f"ğŸ› DEBUG: ã‚«ãƒ¼ãƒ‰ {idx+1} ã®å†…å´JSONè§£ææˆåŠŸ")
                            st.json({
                                'company': job_data.get('company'),
                                'title': job_data.get('title'),
                                'workArea': job_data.get('workArea'),
                                'url': job_data.get('url'),
                                'uniqueId': job_data.get('uniqueId')
                            })
                        
                        # æ±‚äººæƒ…å ±ã‚’æŠ½å‡ºãƒ»æ•´å½¢
                        job_info = {
                            'facility_name': job_data.get('company', job_data.get('siteName', '')),
                            'title': job_data.get('title', job_data.get('formatTitle', '')),
                            'work_area': job_data.get('workArea', ''),
                            'employment_type': job_data.get('employType', ''),
                            'payment': job_data.get('payment', ''),
                            'unique_id': job_data.get('uniqueId', ''),
                            'site_id': job_data.get('siteId', ''),
                            'all_feature_tags': job_data.get('allFeatureTags', []),
                            'original_url': job_data.get('url', ''),  # å…ƒã®ä¼æ¥­URLï¼ˆå‚è€ƒç”¨ï¼‰
                            'rd_url': outer_data.get('uid', ''),  # è©³ç´°ãƒšãƒ¼ã‚¸ã®UID
                        }
                        
                        # åŸºæœ¬çš„ãªä½æ‰€æƒ…å ±
                        job_info['address'] = job_info['work_area']
                        
                        # äº‹æ¥­å†…å®¹ï¼ˆæ±‚äººã‚¿ã‚¤ãƒˆãƒ«ã‹ã‚‰æ¨æ¸¬ï¼‰
                        job_info['business_content'] = job_info['title']
                        
                        # Webã‚µã‚¤ãƒˆURLã¨ã—ã¦æ±‚äººãƒœãƒƒã‚¯ã‚¹ã®è©³ç´°URLã‚’æ§‹ç¯‰
                        unique_id = job_info.get('unique_id', '')
                        if unique_id:
                            job_info['website_url'] = f"{self.base_url}/jb/{unique_id}"
                        else:
                            job_info['website_url'] = ''
                        
                        # æœ€ä½é™ã®æƒ…å ±ãŒæƒã£ã¦ã„ã‚‹å ´åˆã®ã¿ãƒªã‚¹ãƒˆã«è¿½åŠ 
                        if job_info.get('facility_name') and job_info.get('title'):
                            jobs.append(job_info)
                            
                            if debug_mode and idx < 3:
                                st.success(f"ğŸ› DEBUG: ã‚«ãƒ¼ãƒ‰ {idx+1} æƒ…å ±æŠ½å‡ºå®Œäº†")
                                st.json({
                                    'facility_name': job_info['facility_name'],
                                    'title': job_info['title'],
                                    'work_area': job_info['work_area'],
                                    'website_url': job_info['website_url'],
                                    'original_url': job_info['original_url'],
                                    'unique_id': job_info['unique_id']
                                })
                        else:
                            if debug_mode and idx < 3:
                                st.warning(f"ğŸ› DEBUG: ã‚«ãƒ¼ãƒ‰ {idx+1} å¿…è¦ãªæƒ…å ±ãŒä¸è¶³")
                    
                    except json.JSONDecodeError as e:
                        if debug_mode and idx < 3:
                            st.error(f"ğŸ› DEBUG: ã‚«ãƒ¼ãƒ‰ {idx+1} JSONè§£æã‚¨ãƒ©ãƒ¼: {str(e)}")
                        continue
                
                except Exception as e:
                    if debug_mode:
                        st.error(f"ğŸ› DEBUG: ã‚«ãƒ¼ãƒ‰ {idx+1} ã§ã‚¨ãƒ©ãƒ¼: {str(e)}")
                    continue
        
        except Exception as e:
            if debug_mode:
                st.error(f"ğŸ› DEBUG: extract_jobs_from_page ã§ã‚¨ãƒ©ãƒ¼: {str(e)}")
        
        if debug_mode:
            st.info(f"ğŸ› DEBUG: æ±‚äººæƒ…å ±æŠ½å‡ºå®Œäº†: {len(jobs)}ä»¶")
        
        return jobs
    
    def get_job_details(self, detail_url, debug_mode=False):
        """æ±‚äººè©³ç´°ãƒšãƒ¼ã‚¸ã‹ã‚‰è¿½åŠ æƒ…å ±ã‚’å–å¾—"""
        try:
            if debug_mode:
                st.info(f"ğŸ› DEBUG: è©³ç´°ãƒšãƒ¼ã‚¸ã«ã‚¢ã‚¯ã‚»ã‚¹ä¸­: {detail_url}")
            
            response, error = self.make_request(detail_url)
            if error:
                if debug_mode:
                    st.warning(f"ğŸ› DEBUG: è©³ç´°ãƒšãƒ¼ã‚¸ã‚¢ã‚¯ã‚»ã‚¹å¤±æ•—: {error}")
                return {}, error
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            if debug_mode:
                st.info(f"ğŸ› DEBUG: è©³ç´°ãƒšãƒ¼ã‚¸å–å¾—æˆåŠŸ (ã‚µã‚¤ã‚º: {len(response.content)} bytes)")
            
            details = {}
            page_text = soup.get_text()
            
            # æ–½è¨­åï¼ˆä¼šç¤¾åï¼‰ã‚’è©³ç´°ãƒšãƒ¼ã‚¸ã‹ã‚‰å–å¾—
            facility_patterns = [
                re.compile(r'(?:ä¼šç¤¾å|æ³•äººå|æ–½è¨­å|ç—…é™¢å|ã‚¯ãƒªãƒ‹ãƒƒã‚¯å|äº‹æ¥­æ‰€å|ä¼æ¥­å)[:ï¼š]?\s*(.+?)(?:\n|$)', re.MULTILINE),
                re.compile(r'(?:å‹¤å‹™å…ˆ|é‹å–¶ä¼šç¤¾|é‹å–¶æ³•äºº)[:ï¼š]?\s*(.+?)(?:\n|$)', re.MULTILINE),
            ]
            
            for pattern in facility_patterns:
                facility_matches = pattern.findall(page_text)
                if facility_matches:
                    facility_name = facility_matches[0].strip()[:100]
                    if len(facility_name) > 2:
                        details['facility_name'] = facility_name
                        if debug_mode:
                            st.success(f"ğŸ› DEBUG: æ–½è¨­åç™ºè¦‹: {facility_name}")
                        break
            
            # ä»£è¡¨è€…åã‚’è©³ç´°ãƒšãƒ¼ã‚¸ã‹ã‚‰å–å¾—
            representative = self.extract_representative(page_text)
            if representative:
                details['representative'] = representative
                if debug_mode:
                    st.success(f"ğŸ› DEBUG: ä»£è¡¨è€…åç™ºè¦‹: {representative}")
            
            # é›»è©±ç•ªå·ã®æŠ½å‡ºï¼ˆã‚ˆã‚ŠåŒ…æ‹¬çš„ï¼‰
            phone_patterns = [
                re.compile(r'(?:é›»è©±|TEL|Tel|tel)[:ï¼š]?\s*(\d{2,4}[-â€\s]?\d{2,4}[-â€\s]?\d{4})'),  # é›»è©±: å½¢å¼
                re.compile(r'(\d{2,4}[-â€]\d{2,4}[-â€]\d{4})'),  # ä¸€èˆ¬çš„ãªé›»è©±ç•ªå·
                re.compile(r'(\d{10,11})'),  # ãƒã‚¤ãƒ•ãƒ³ãªã—
            ]
            
            for pattern in phone_patterns:
                phone_matches = pattern.findall(page_text)
                if phone_matches:
                    phone = phone_matches[0]
                    details['phone'] = phone
                    if debug_mode:
                        st.success(f"ğŸ› DEBUG: é›»è©±ç•ªå·ç™ºè¦‹: {phone}")
                    break
            
            # ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹ã®æŠ½å‡º
            email_patterns = [
                re.compile(r'(?:ãƒ¡ãƒ¼ãƒ«|E-mail|Email|email)[:ï¼š]?\s*([a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,})'),
                re.compile(r'([a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,})'),
            ]
            
            for pattern in email_patterns:
                email_matches = pattern.findall(page_text)
                if email_matches:
                    # .pngã‚„.jpgãªã©ã®ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã‚’é™¤å¤–
                    valid_emails = [em for em in email_matches if not any(ext in em.lower() for ext in ['.png', '.jpg', '.jpeg', '.gif', '.css', '.js'])]
                    if valid_emails:
                        details['email'] = valid_emails[0]
                        if debug_mode:
                            st.success(f"ğŸ› DEBUG: ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹ç™ºè¦‹: {valid_emails[0]}")
                        break
            
            # ä½æ‰€æƒ…å ±ã‚’è©³ç´°ãƒšãƒ¼ã‚¸ã‹ã‚‰å–å¾—
            address_patterns = [
                re.compile(r'(?:ä½æ‰€|æ‰€åœ¨åœ°|Address)[:ï¼š]?\s*(.+?[éƒ½é“åºœçœŒ].+?[å¸‚åŒºç”ºæ‘].+?)(?:\n|<|$)', re.MULTILINE),
                re.compile(r'([æ±äº¬éƒ½|å¤§é˜ªåºœ|äº¬éƒ½åºœ|åŒ—æµ·é“|é’æ£®çœŒ|å²©æ‰‹çœŒ|å®®åŸçœŒ|ç§‹ç”°çœŒ|å±±å½¢çœŒ|ç¦å³¶çœŒ|èŒ¨åŸçœŒ|æ ƒæœ¨çœŒ|ç¾¤é¦¬çœŒ|åŸ¼ç‰çœŒ|åƒè‘‰çœŒ|ç¥å¥ˆå·çœŒ|æ–°æ½ŸçœŒ|å¯Œå±±çœŒ|çŸ³å·çœŒ|ç¦äº•çœŒ|å±±æ¢¨çœŒ|é•·é‡çœŒ|å²é˜œçœŒ|é™å²¡çœŒ|æ„›çŸ¥çœŒ|ä¸‰é‡çœŒ|æ»‹è³€çœŒ|å…µåº«çœŒ|å¥ˆè‰¯çœŒ|å’Œæ­Œå±±çœŒ|é³¥å–çœŒ|å³¶æ ¹çœŒ|å²¡å±±çœŒ|åºƒå³¶çœŒ|å±±å£çœŒ|å¾³å³¶çœŒ|é¦™å·çœŒ|æ„›åª›çœŒ|é«˜çŸ¥çœŒ|ç¦å²¡çœŒ|ä½è³€çœŒ|é•·å´çœŒ|ç†Šæœ¬çœŒ|å¤§åˆ†çœŒ|å®®å´çœŒ|é¹¿å…å³¶çœŒ|æ²–ç¸„çœŒ].+?[å¸‚åŒºç”ºæ‘].+?)(?:\n|$)', re.MULTILINE),
                re.compile(r'(?:ä½æ‰€|æ‰€åœ¨åœ°|å‹¤å‹™åœ°)[:ï¼š\s]*([^\n\r]+)'),
            ]
            
            for pattern in address_patterns:
                address_matches = pattern.findall(page_text)
                if address_matches:
                    address = address_matches[0].strip()[:200]  # æœ€å¤§200æ–‡å­—
                    if len(address) > 5:  # ä½æ‰€ã¨ã—ã¦å¦¥å½“ãªé•·ã•
                        details['address'] = address
                        if debug_mode:
                            st.success(f"ğŸ› DEBUG: ä½æ‰€ç™ºè¦‹: {address[:50]}...")
                        break
            
            # ã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆURLã®æŠ½å‡ºï¼ˆå¤§å¹…ã«æ”¹å–„ï¼‰
            website_patterns = [
                re.compile(r'(?:ãƒ›ãƒ¼ãƒ ãƒšãƒ¼ã‚¸|Website|URL|HP|å…¬å¼ã‚µã‚¤ãƒˆ)[:ï¼š]?\s*(https?://[^\s\n<>\"\']+)'),
                re.compile(r'(?:è©³ç´°ã¯|è©³ã—ãã¯).*(https?://[^\s\n<>\"\']+)'),
                re.compile(r'(https?://(?:www\.)?[a-zA-Z0-9][-a-zA-Z0-9]{0,62}(?:\.[a-zA-Z0-9][-a-zA-Z0-9]{0,62})*\.(?:com|co\.jp|jp|org|net|info)/[^\s\n<>\"\']*)', re.IGNORECASE),
                re.compile(r'(https?://(?:www\.)?[a-zA-Z0-9][-a-zA-Z0-9]{0,62}(?:\.[a-zA-Z0-9][-a-zA-Z0-9]{0,62})*\.(?:com|co\.jp|jp|org|net|info))', re.IGNORECASE),
            ]
            
            # é™¤å¤–ã™ã¹ããƒ‰ãƒ¡ã‚¤ãƒ³ï¼ˆã‚ˆã‚ŠåŒ…æ‹¬çš„ï¼‰
            exclude_domains = [
                'xn--pckua2a7gp15o89zb.com',  # æ±‚äººãƒœãƒƒã‚¯ã‚¹è‡ªä½“
                'google.com', 'google.co.jp',
                'yahoo.co.jp', 'yahoo.com',
                'youtube.com', 'youtu.be',  # YouTube
                'facebook.com', 'fb.com',
                'twitter.com', 'x.com',
                'instagram.com',
                'linkedin.com',
                'tiktok.com',
                'line.me',
                'amazon.co.jp', 'amazon.com',
                'rakuten.co.jp',
                'mercari.com',
                'indeed.com', 'indeed.co.jp',
                'rikunabi.com', 'mynavi.jp',
                'en-japan.com', 'doda.jp',
                'cookpad.com',
                'wikipedia.org',
                'cloudfront.net',
                'googleapis.com',
                'gstatic.com'
            ]
            
            for pattern in website_patterns:
                website_matches = pattern.findall(page_text)
                if website_matches:
                    # æœ‰åŠ¹ãªã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆã‚’æŠ½å‡º
                    valid_websites = []
                    for url in website_matches:
                        # URLæ­£è¦åŒ–
                        clean_url = url.rstrip('.,;:)')
                        
                        # é™¤å¤–ãƒ‰ãƒ¡ã‚¤ãƒ³ãƒã‚§ãƒƒã‚¯
                        is_excluded = any(domain in clean_url.lower() for domain in exclude_domains)
                        
                        # ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã‚„ç„¡åŠ¹ãªãƒ‘ã‚¹ã®é™¤å¤–
                        is_invalid_file = any(ext in clean_url.lower() for ext in ['.png', '.jpg', '.jpeg', '.gif', '.css', '.js', '.pdf'])
                        
                        # æœ‰åŠ¹ãªURLã®å ´åˆã®ã¿è¿½åŠ 
                        if not is_excluded and not is_invalid_file and len(clean_url) > 10:
                            valid_websites.append(clean_url)
                    
                    if valid_websites:
                        # æœ€ã‚‚ä¼æ¥­ã‚µã‚¤ãƒˆã‚‰ã—ã„URLã‚’é¸æŠ
                        best_url = None
                        for url in valid_websites:
                            # co.jp ãƒ‰ãƒ¡ã‚¤ãƒ³ã‚’å„ªå…ˆ
                            if '.co.jp' in url.lower():
                                best_url = url
                                break
                            # .jp ãƒ‰ãƒ¡ã‚¤ãƒ³ã‚’æ¬¡ã«å„ªå…ˆ
                            elif '.jp' in url.lower() and not best_url:
                                best_url = url
                            # ãã®ä»–ã®ãƒ‰ãƒ¡ã‚¤ãƒ³ã¯æœ€å¾Œ
                            elif not best_url:
                                best_url = url
                        
                        if best_url:
                            details['website_url'] = best_url
                            if debug_mode:
                                st.success(f"ğŸ› DEBUG: ã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆURLç™ºè¦‹: {best_url}")
                                if len(valid_websites) > 1:
                                    st.info(f"ğŸ› DEBUG: ä»–ã®å€™è£œ: {valid_websites[1:]}")
                            break
            
            # äº‹æ¥­å†…å®¹ã®æŠ½å‡º
            business_patterns = [
                re.compile(r'(?:äº‹æ¥­å†…å®¹|æ¥­å‹™å†…å®¹|äº‹æ¥­æ¦‚è¦|Business)[:ï¼š]?\s*(.+?)(?:\n\n|\n[A-Z]|\n\d+\.|\nâ€¢|\n-|$)', re.MULTILINE | re.DOTALL),
                re.compile(r'(?:æ¥­ç¨®|äº‹æ¥­|Business)[:ï¼š]?\s*(.+?)(?:\n|$)', re.MULTILINE),
            ]
            
            for pattern in business_patterns:
                business_matches = pattern.findall(page_text)
                if business_matches:
                    business_content = business_matches[0].strip()[:500]  # æœ€å¤§500æ–‡å­—
                    if len(business_content) > 10:  # 10æ–‡å­—ä»¥ä¸Šã®å ´åˆã®ã¿æ¡ç”¨
                        details['business_content'] = business_content
                        if debug_mode:
                            st.success(f"ğŸ› DEBUG: äº‹æ¥­å†…å®¹ç™ºè¦‹: {business_content[:50]}...")
                        break
            
            return details, None
            
        except Exception as e:
            error_msg = f"è©³ç´°æƒ…å ±å–å¾—ã‚¨ãƒ©ãƒ¼: {str(e)}"
            if debug_mode:
                st.error(f"ğŸ› DEBUG: {error_msg}")
            return {}, error_msg
    
    def scrape_jobs(self, employment_type_word, keyword, area, max_jobs=10, progress_callback=None, debug_mode=False):
        """æ±‚äººæƒ…å ±ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ï¼ˆæ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ä½¿ç”¨ï¼‰"""
        try:
            # æ¤œç´¢URLã‚’ç”Ÿæˆ
            search_url = self.generate_search_url(employment_type_word, keyword, area)
            if not search_url:
                error_msg = "æ¤œç´¢URLã®ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸ"
                if debug_mode:
                    st.error(f"ğŸ› DEBUG: {error_msg}")
                    st.code(f"employment_type_word: {employment_type_word}\nkeyword: {keyword}\narea: {area}")
                return None, error_msg
            
            if debug_mode:
                st.info(f"ğŸ› DEBUG: ç”Ÿæˆã•ã‚ŒãŸæ¤œç´¢URL: {search_url}")
                st.info("ğŸ› DEBUG: æ¤œç´¢çµæœãƒšãƒ¼ã‚¸ã‹ã‚‰JSONæ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ã‚’ç›´æ¥æŠ½å‡º")
            
            all_jobs = []
            page_num = 1
            max_pages = min(10, (max_jobs // 10) + 1)
            
            # æ¤œç´¢çµæœãƒšãƒ¼ã‚¸ã‹ã‚‰ç›´æ¥JSONæ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º
            while page_num <= max_pages and len(all_jobs) < max_jobs:
                # ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³å¯¾å¿œ
                if page_num == 1:
                    current_url = search_url
                else:
                    if "?" in search_url:
                        current_url = search_url + f"&p={page_num}"
                    else:
                        current_url = search_url + f"?p={page_num}"
                
                if debug_mode:
                    st.info(f"ğŸ› DEBUG: ãƒšãƒ¼ã‚¸ {page_num} ã¸ã‚¢ã‚¯ã‚»ã‚¹ä¸­: {current_url}")
                
                response, error = self.make_request(current_url)
                if error:
                    if debug_mode:
                        st.error(f"ğŸ› DEBUG: ãƒšãƒ¼ã‚¸ {page_num} ã§ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ: {error}")
                    if page_num == 1:
                        return None, error
                    else:
                        break
                
                soup = BeautifulSoup(response.content, 'html.parser')
                
                # JSONæ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰æ±‚äººæƒ…å ±ã‚’æŠ½å‡º
                jobs = self.extract_jobs_from_page(soup, current_url, debug_mode=debug_mode)
                
                if debug_mode:
                    st.info(f"ğŸ› DEBUG: ãƒšãƒ¼ã‚¸ {page_num} ã‹ã‚‰ {len(jobs)} ä»¶ã®æ±‚äººã‚’æŠ½å‡º")
                
                if not jobs:
                    if debug_mode:
                        st.warning(f"ğŸ› DEBUG: ãƒšãƒ¼ã‚¸ {page_num} ã§æ±‚äººãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
                    break
                
                # å„æ±‚äººã«å¯¾ã—ã¦Webæ¤œç´¢ã§è£œå®Œæƒ…å ±ã‚’å–å¾—
                for idx, job in enumerate(jobs):
                    if len(all_jobs) >= max_jobs:
                        break
                    
                    if progress_callback:
                        progress_callback(len(all_jobs) + 1, max_jobs)
                    
                    if debug_mode and len(all_jobs) < 3:
                        st.info(f"ğŸ› DEBUG: æ±‚äºº {len(all_jobs) + 1} ã‚’å‡¦ç†ä¸­: {job.get('facility_name', 'Unknown')}")
                    
                    # Webæ¤œç´¢ã§é€£çµ¡å…ˆæƒ…å ±ã‚’å–å¾—
                    if job.get('facility_name'):
                        if debug_mode and len(all_jobs) < 2:  # æœ€åˆã®2ä»¶ã®ã¿è©³ç´°ãªWebæ¤œç´¢ãƒ­ã‚°ã‚’è¡¨ç¤º
                            st.info(f"ğŸ” æ–½è¨­å '{job['facility_name']}' ã®Webæ¤œç´¢ã‚’é–‹å§‹...")
                        
                        contact_info = self.search_contact_info_web(
                            job['facility_name'], 
                            debug_mode=(debug_mode and len(all_jobs) < 2)
                        )
                        job.update(contact_info)
                    
                    # ãƒ‡ãƒ¼ã‚¿ã‚’æ•´å½¢
                    formatted_job = {
                        'facility_name': job.get('facility_name', ''),
                        'representative': job.get('representative', ''),
                        'address': job.get('address', job.get('work_area', '')),
                        'website_url': job.get('website_url', ''),  # æ±‚äººãƒœãƒƒã‚¯ã‚¹ã®è©³ç´°URL
                        'phone_number': job.get('phone', ''),
                        'email': job.get('email', ''),
                        'business_content': job.get('business_content', job.get('title', '')),
                        'source_url': job.get('website_url', current_url)  # æ±‚äººãƒœãƒƒã‚¯ã‚¹ã®è©³ç´°URL
                    }
                    
                    all_jobs.append(formatted_job)
                    
                    if debug_mode and len(all_jobs) <= 3:
                        st.success(f"ğŸ› DEBUG: æ±‚äºº {len(all_jobs)} å‡¦ç†å®Œäº†")
                        st.json({
                            'facility_name': formatted_job['facility_name'],
                            'address': formatted_job['address'],
                            'phone_number': formatted_job['phone_number'],
                            'email': formatted_job['email'],
                            'representative': formatted_job['representative'],
                            'website_url': formatted_job['website_url'],
                            'business_content': formatted_job['business_content'][:100] + '...' if len(formatted_job['business_content']) > 100 else formatted_job['business_content']
                        })
                
                # æ¬¡ã®ãƒšãƒ¼ã‚¸ã¸
                page_num += 1
                time.sleep(1)  # ãƒ¬ãƒ¼ãƒˆåˆ¶é™
            
            if debug_mode:
                st.success(f"ğŸ› DEBUG: ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Œäº† - æœ€çµ‚çš„ã« {len(all_jobs)} ä»¶ã®æ±‚äººæƒ…å ±ã‚’å–å¾—")
            
            return all_jobs, None
            
        except Exception as e:
            error_msg = f"ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}"
            if debug_mode:
                st.error(f"ğŸ› DEBUG: {error_msg}")
                st.code(f"ä¾‹å¤–è©³ç´°: {type(e).__name__}: {str(e)}")
            return None, error_msg


class KyujinboxUI:
    """æ±‚äººãƒœãƒƒã‚¯ã‚¹ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ã®UI"""
    
    def __init__(self):
        self.scraper = KyujinboxScraper()
    
    def render_ui(self):
        """UIã‚’æç”»ã—ã¦çµæœã‚’è¿”ã™"""
        st.header("æ±‚äººãƒœãƒƒã‚¯ã‚¹ï¼šæ¤œç´¢æ¡ä»¶")
        
        # ãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰
        debug_mode = st.sidebar.checkbox("ãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰", key="kyujinbox_debug")
        
        # é›‡ç”¨å½¢æ…‹ã®é¸æŠè‚¢ã‚’å‹•çš„å–å¾—
        employment_types = self.scraper.get_employment_types()
        employment_options = [emp['text'] for emp in employment_types]
        
        # é›‡ç”¨å½¢æ…‹é¸æŠ
        selected_employment = st.selectbox(
            "é›‡ç”¨å½¢æ…‹",
            employment_options,
            key="kyujinbox_employment_type"
        )
        
        # é¸æŠã•ã‚ŒãŸé›‡ç”¨å½¢æ…‹ã®data-search_wordã‚’å–å¾—
        selected_employment_word = ""
        for emp in employment_types:
            if emp['text'] == selected_employment:
                selected_employment_word = emp['data_search_word']
                break
        
        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å…¥åŠ›
        keyword = st.text_input(
            "ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ (è·ç¨®ã€æ¥­ç¨®ãªã©)",
            placeholder="çœ‹è­·å¸«ã€ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãªã©",
            key="kyujinbox_keyword"
        )
        
        # ã‚¨ãƒªã‚¢å…¥åŠ›
        area = st.text_input(
            "ã‚¨ãƒªã‚¢ (éƒ½é“åºœçœŒã€å¸‚åŒºç”ºæ‘ã€é§…å)",
            placeholder="æ±äº¬éƒ½ã€æ–°å®¿åŒºã€æ¸‹è°·é§…ãªã©",
            key="kyujinbox_area"
        )
        
        # å–å¾—ä»¶æ•°è¨­å®š
        max_jobs = st.selectbox(
            "å–å¾—ä»¶æ•°ã‚’é¸æŠ",
            [10, 20, 30, 50, 100],
            index=0,  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§10ä»¶
            key="kyujinbox_max_jobs",
            help="å–å¾—ã™ã‚‹æ±‚äººã®æœ€å¤§ä»¶æ•°ã‚’é¸æŠã—ã¦ãã ã•ã„ï¼ˆä¸Šé™100ä»¶ï¼‰"
        )
        
        # å®Ÿè¡Œãƒœã‚¿ãƒ³
        start_button = st.button(
            "ã“ã®æ¡ä»¶ã§æ¤œç´¢ã™ã‚‹",
            type="primary",
            key="kyujinbox_start"
        )
        
        # æ¤œç´¢å®Ÿè¡Œ
        if start_button:
            if not any([selected_employment_word, keyword, area]):
                st.warning("é›‡ç”¨å½¢æ…‹ã€ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã€ã‚¨ãƒªã‚¢ã®ã†ã¡å°‘ãªãã¨ã‚‚1ã¤ã¯å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
                return None
            
            if debug_mode:
                st.info(f"ğŸ› DEBUG: é–‹å§‹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ - é›‡ç”¨å½¢æ…‹: {selected_employment_word}, ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {keyword}, ã‚¨ãƒªã‚¢: {area}, å–å¾—æ•°: {max_jobs}")
            
            with st.spinner("æ±‚äººæƒ…å ±ã‚’å–å¾—ä¸­... ã—ã°ã‚‰ããŠå¾…ã¡ãã ã•ã„"):
                # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼
                progress_bar = st.progress(0)
                status_text = st.empty()
                
                def progress_callback(current, total):
                    if total > 0:
                        progress_bar.progress(min(current / total, 1.0))
                    status_text.text(f"æ±‚äººæƒ…å ±ã‚’å–å¾—ä¸­... ({current}/{total})")
                
                # æ±‚äººæƒ…å ±å–å¾—
                job_list, error = self.scraper.scrape_jobs(
                    selected_employment_word, 
                    keyword, 
                    area,
                    max_jobs,
                    progress_callback,
                    debug_mode
                )
                
                # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼ã‚’ã‚¯ãƒªã‚¢
                progress_bar.empty()
                status_text.empty()
                
                if error:
                    st.error(error)
                    if debug_mode:
                        st.code(f"ã‚¨ãƒ©ãƒ¼è©³ç´°: {error}")
                    return None
                
                if not job_list:
                    st.warning("æ±‚äººæƒ…å ±ã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚æ¤œç´¢æ¡ä»¶ã‚’å¤‰æ›´ã—ã¦ãŠè©¦ã—ãã ã•ã„ã€‚")
                    if debug_mode:
                        st.info("ğŸ› DEBUG: ç©ºã®çµæœãŒè¿”ã•ã‚Œã¾ã—ãŸ")
                    return None
                
                # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã«å¤‰æ›
                df_data = []
                for job in job_list:
                    df_data.append({
                        'æ–½è¨­å': job['facility_name'],
                        'ä»£è¡¨è€…å': job['representative'],
                        'ä½æ‰€': job['address'],
                        'Webã‚µã‚¤ãƒˆURL': job['website_url'],
                        'é›»è©±ç•ªå·': job['phone_number'],
                        'ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹': job['email'],
                        'äº‹æ¥­å†…å®¹': job['business_content']
                    })
                
                df = pd.DataFrame(df_data)
                
                if debug_mode:
                    st.success(f"ğŸ› DEBUG: ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ä½œæˆå®Œäº† - {len(df)}è¡Œ, {len(df.columns)}åˆ—")
                
                st.success(f"{len(df)}ä»¶ã®æ±‚äººæƒ…å ±ã‚’å–å¾—ã—ã¾ã—ãŸã€‚")
                
                return df
        
        else:
            st.info("æ¤œç´¢æ¡ä»¶ã‚’å…¥åŠ›ã—ã¦æ±‚äººã‚’æ¢ã—ã¦ãã ã•ã„ã€‚")
            return None